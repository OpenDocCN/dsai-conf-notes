# 2024北京智源大会-大语言模型 - P4：大语言模型知识机理与编辑问题：张宁豫 - 智源社区 - BV1zE421N7UJ

我是来自浙大的张宁玉啊，然后呃我今天介绍的这个topic，其实前面两个老师都有这个关注过，就是提到过，就是啊我们现在都知道大运行语言模型啊，它给我们带来很大的冲击啊。

它的效果其实在很方面都很多方面都非常好，很多以前的自然语言任务啊，基本上它的效果都已经达到了，一个非常高的一个高度，但它背后的这个原理又是什么啊，其实我今天这个PPT题目应该加两个字啊。

应该叫加上假说啊，因为我们团队其实在这方面，其实做了一些的这个探索和思考啊，但其实背后呢呃很难讲清楚，这个到底是不是一个真正的一个原理，或者真理啊，其实我们最后讲的很多都是假说。

那我今天主要会分析一下这个大型语言模型，从知识的这个视角，它背后的这个机理啊，可能是什么啊，以及我们如何去操作它背后的知识，也就是去编辑它的背后的这个知识啊。



![](img/023b9ed505a39fe86fa1685eee1d0822_1.png)

那我主要介绍两个问题呃，一个就是语言模型，它去存储以及表达啊，我们的这些这个啊像人类的一些很多的知识，那语言模型到底是如何去存储跟表达，这类知识呢，啊这个是第一个问题，那么在这个如果这个问题能够回答好。

或者是能够有一些初步理解的话，那我们进一步又如何去比较精准的高效的去啊，更新它里边的这个知识呢，这是我今天可能要啊介绍的第二个问题啊，其实第二个问题也能够促进我们去实现，更加可信可靠的一些这个啊应用。



![](img/023b9ed505a39fe86fa1685eee1d0822_3.png)

那围绕这个方面啊，其实呃我们都知道，这个其实有很多实际的应用啊，比如说这个语言模型，它有些时候它会有一些知识是过时的啊，还有很多啊有偏见的，甚至是有一些有毒的信息，这些其实这些问题。

其实对于我们实际的大模型的应用，其实带来了很多的这个困扰，所以有很多很多的这个呃技术一直在探讨，这个如何去更新，或者是如何去啊，修正语言模型中的这个知识谬误，这些背后的一个本质的问题。

还是我们需要理解语言模型中的这个知识，它到底是如何的去存储跟表达的，其实在这个方面啊，其实像呃我们国外的很多的这个公司。



![](img/023b9ed505a39fe86fa1685eee1d0822_5.png)

他们其实已经在做一些很初步的一些探讨了，比如说像ASSOPPIC，他们前段时间啊有一篇工作啊，就是从可解释的角度啊，分析了这个像啊cloudy的一些大型模型中，它这个在呃里边的一些区域。

到底是如何存储一些表征，一些相关的一些知识的信息的啊，他们甚至把这些相关的技术应用在一些啊，安全啊，安全的模型更加安全的一些领域，那么其实open i在前段时间也做了一篇啊，放出来一篇文章啊。

他们其实是啊，通过一些sps out to encoding的方式啊，提取扫描了其中的啊带海量的特征，其中很多特征其实都跟我们啊人类所熟知的一，些knowledge是非常相关的啊。

它从里边特征的角度去发现，里边有一些特征，可能表致了某某一部分的这个信息，那么从更深层次的角度啊，原模型到底是如何去表征这些复杂的。



![](img/023b9ed505a39fe86fa1685eee1d0822_7.png)

这种知识呢，啊那么我们从这个分析一个事物的一个角度啊，其实有两个呃维度，一个是从底向上，也就是说从神经元底层开始去分析。



![](img/023b9ed505a39fe86fa1685eee1d0822_9.png)

还有一个角度是从顶向下，那么从底向上的这种做法呢，其实是啊从哲学领域叫做还原论的这种思想，之前有很多工作都在思考，就是说啊我们其实可以去分析分析，这个神经元或者这一个组件到底是什么，围绕这个视角。

其实前期已经有很多学者提出了，一些非常优秀的一些假说，比如说像知识神经元的这种假说啊，比如说某一些神经元，或者是transformer中的MLP，某一些layer可能表征了某一个事实的这种知识。

那么其实还有这里边存在一个问题，就是我们想象一下，其实呃知识是非常复杂的，他们知识与知识之间，其实存在很多很多的这个关联啊，其实像左边这个图的这个有一篇去年的呃，前两年的一篇science s工作里。

他其实发现人类人脑在思考或者是做一些记忆，或者一些深度思考的时候啊，不同的区域之间是有一些linkage，有些关联的，那么语言模型，它的这个区域之间，是不是也存在一个明显的关联性，我们是不是可以啊。

从一个整体论的视角，从自顶向下来去分析，这个知识到底是如何去存储跟表征的啊，那围绕这个视角的话，其实我们团队最近做了一个啊，比较初步的一个思考啊，就是我们从这个整体论的视角来思考这个知识，是不是可以啊。

从整体的视角我们把它叫做这种知识的回路，那么这个回路其实啊啊是其实是一个很古老，很早就有的概念，在可解释领域，就是以前在解释这个原模型，包括很多深度学习框架里边，有一些很多学者他们提出了啊。

可以去造一个特定行为的一个子图，来去解释这个原模型这些行为。

![](img/023b9ed505a39fe86fa1685eee1d0822_11.png)

那么我们其实做了一个视角，就是我们从这个知识的维度啊，来去提出了一个知识回路的一个这个假说啊，其实这个词其实老早就有啊，我记得好像去年啊张俊领导上提过这个词，其实啊在很多其他领域。

也有知识回路的这个说法，那么语言模型特别是大型语言模型，他的这个知识回路到底长的什么样子啊，那这张图其实就是有一个动图，就是可啊比较形象的把它展示出来，我们其实发现围绕一个事实知识。

可能中间有一些比较关键的MLP层，或者是attention层，或者是一些其他的一些组件，甚至是一些node等等，这些组件有一些组件表达了一个关系，有一些组件表达了某一个实体。

有一些组件可能表达了些其他的东西啊，甚至我们人类还不一定能理解这些东西，但他们共同一起来运作，然后最后来去实现了这一条知识的一个表达，也就是说可能啊这个通过这种模块化的组合，而不是单个神经元。

而是多个神经元之间共同的一个组合，去完成了一条知识的一个表达，那么具体的其实里边有很多，这是一个呃我们团队在这个GPT2啊，因为这个太大了，模型高效不太好去做，我们就围绕像GPT2。

甚至是tiny la等等模型去啊挖掘，去分析它的一些回路，那这个就是一个啊真实的一个在GBT2中的，一条事实知识的一个这个回路，然后右边这个图呢是这个回路的简化版本，我们可以发现啊。

其中啊有大量的MLP，也就这个就是那个FINN层，也刚好也印证了以前的这个知识神经元的假说，MLP层其实对于很多视知识是非常重要的，但其实里边有很多像其他层的，包括注意力等等。

这些其他的一些这种组件的信息，也就说明在表征这条知识的时候，其实有很多很多组件共同一起协作，来完成了这条知识的一个表达，那么这里边有很多关键的组件，比如说像这个move head啊。

它可能我们其实发现可能啊，它其实把这个投实体的信息，通过这个move head，把它转移到了这个下一个叫LETOKEN的等等信息，还有一些像relating head的。

我们其实发现了很多啊知识共享的reacing head，有一些head可能表征了很多很多共同的一些。

![](img/023b9ed505a39fe86fa1685eee1d0822_13.png)

这种事实的这个知识啊，这也是比较有趣的一些这个发现啊，那我们其实围绕这个发现的这个啊这个回路，我们就去干脆做了一些实验，我们发现纯用这个发现的这个回路，其实就能够维持大约70%的这个模型的，这个性能。

甚至是我们拿这个回路去带一些其他的，就是跟它相相关的一些测试集上，甚至还有一定的提升，这也说明了，其实回路可能表征了已经表征了这个老知识，非常很多，很多组成部分就是靠这条回路来表征的。

甚至其他部分能还起到了一个副作用，这也是我们在猜想，为什么在测试集上还有提升的，一个可能的一个因素啊，那除了这个之外呢，我们进一步的去分析这个啊。



![](img/023b9ed505a39fe86fa1685eee1d0822_15.png)

有一些呃我们都知道，众所周知的大模型的一些现象到底是怎么回事，我们其实探讨了几个问题，一个是幻觉问题，在幻觉情况下，这个回路到底是怎么回事啊，我们发现了一个有趣的现象，就是幻觉情况下。

这个回路可能是个错的，就比如说像左边这张图里边啊，在这个L15这层啊，出现了一条错误的一个流向信息，导致了这个啊某一个值急剧的下降，也就是说可能是因为这个这条知识，之所以有幻觉。

是因为L15H0这个节点，直接导致了整个模型就留下了一个错误信息，但是对于一些正常的事实来说，它是没有经过这个错误流向的，它是流向另外一条这个节点的啊，当然这只是个假说啊。

这是我们只是在实验中发现的一些这个现象，另外一个比较有趣的。

![](img/023b9ed505a39fe86fa1685eee1d0822_17.png)

我个人觉得比较有趣的现象是在in contest learning的，这个时候，就我们都知道in contest learning，甚至现代的很多的这种red啊，我们加一些prompt的这个模型。

好像就一下子就会了这个答案了，就感觉很多新知识就会了，那它背后的原理到底是什么，那右边这张图呢，就是我们当时在做回路分析的时候，发现了一个有趣的现象，就说加了这些prompt以后，或者DEMC信以后。

这个突然之间冒出了一些特定的注意力头啊，这些注意力头关注到了这个demo session的信息，然后是让这个模型走向了正确的这个答案啊，这个也是一个有趣的一个发现啊，其实我印象中其实同期还有一篇工作啊。

把这个注意的可能称之为检索头啊，其实都是一些类似的现象，就说这个回路中是不是会有一些特定的信息，它会关注这个ICL的一些弹幕STRATION，然后让这个模型能够去啊。

相当于就临时就获得了这个新的这个知识等等，这都是一些些有趣的这个现象好。

![](img/023b9ed505a39fe86fa1685eee1d0822_19.png)

那么啊有了这个现象以后啊，其实我们就可以有一个初步假设啊，原模型中间的这个知识，可能还是有一些这个规律的，那么这个规律可能能够帮助我们去解释它的，一些这个现象啊，那我如何去这个啊，在这些规律下。

如何让去更新，如何去更新，以及这个呃去精准的去操作这些中间的知识呢，那就是我今天要探讨的这个啊。

![](img/023b9ed505a39fe86fa1685eee1d0822_21.png)

第二个问题就是五大模型的这个，知识的这个编辑啊，其实编辑这个问题很难啊，其实有很多同行都在跟我交流的时候，都考虑到一个问题，就说我们如果去改了这个模型中的一些问题，是不是这个模型就坏掉了。

这个模型可能就根本就不work了啊，但这里边其实有很多很多值得探讨的问题，假如说我们能够去理解它背后的，这个机理的话啊，其实我们可以在一个很小的待定代价，代价下去更新。

或者是擦除它中间的一些这种knowledge，来更好的去啊服务的这个应用，那我们团队最近其实做了一些这些思考，做了一个比较新的工作啊，就是我们考虑到一个情况，就是原模型。

其实啊我们在更新它的这个知识的时候，不可避免它肯定会影响，那么我们就在想这个怎么样去，更高效的去尽量避免这个模型在更新的时候，对它原有性能的一个这个影响，那我们就考虑去借鉴这个啊。

人在认知的时候的一个过程啊，人在认知的过程的时候，他的记忆其实是存在一个工作记忆，和一个这种长期记忆的，一个一个一个一个一个区分的啊，就比如说啊，其实啊今天各位来参加这个纸园大会对吧。

我们可能今天听了这个报告，可能大家还今天能够记得很清楚，那过段这个其实就是将来当天的一个呃working memory，那么可能有些信息呢，经过啊大量的这种这种消化吸收以后。

可能就固化在啊你的这个长期记忆里边，那就可能进行了一个长期的这个记忆，那么我们其实类比这个核心那个想法啊，我们其实在做编辑的时候，如果去直接去修改这个模型的核心参数，本身，它会影响到模型的长期记忆。

可能会对这个模型的performance，产生一个非常这个重大的一个影响，就有可能把模型搞崩掉了，那么我们就尝试想，是不是可以给大模型去做一个工作的，一个记忆啊，而去直接操作这个工作记忆。

来去实现这种模型的一个自我的一个编辑，和长期的一个更新啊，这样子也可以更好的去避免它的一，些这个副作用，那么我们最近就做了一个探索啊。



![](img/023b9ed505a39fe86fa1685eee1d0822_23.png)

就提了一个工作叫做这个wise啊，这个工作其实思路非常简单，也是啊结合了刚才所提的知识机理的这个假说，中的这个MOP，可能代表了很多比较重要的知识，那我们对对这个MOP这层啊，从左往右看，这个MLP。

这个绿色就是它原有的这个原模型的，这个支持的这个记忆的一个核心的一个区域，那我们旁边给他造了一个旁支回路，这个回路其实就是我们把它假称称为工作，工作工作的一个记忆，它可能是一个临时的一个记忆啊。

这个新知识来了以后，我们把它给存储在这个MLP的旁支的，这个回路里边，那这个回路的原始的参数，是从这个绿色部分拷贝过来的，相当于它是啊基于已有的这个知识，然后我们去对它进行这个蓝色区域。

进行一个啊更新啊，那么这样的话我们再设计一个门控的机制，来，去决定它什么时候是使用以前的固有的，这个记忆来，什么时候使用这个长期的工作记忆，这样就可以保证模型在持续时间很长的一段，知识的编辑的过程中。

它的固有记忆是不会发生太大变化的，同时他也学会了一些新的一些这种知识。

![](img/023b9ed505a39fe86fa1685eee1d0822_25.png)

那具体啊其实这里边有一些技术细节啊，就是我们如何去让他这个学习到新的知识呢，我们又把这个知识啊进行了若干个，这个随机的一个这种分区啊，这个经历形象起来非常简单，就比如说我们都知道呃，这个比如说前天昨天。

今天我各位大家都会发生不同的事情，经历不同的事情，那我们对每一个time step的情况下，都给他啊，copy1份这个相关的一个知识的一个分区啊，每个分区都是从原始的这个MFI，MMLOP那个层来的。

那么每个分区都可以去更新它当前状态的，这个知识啊，然后都会得到了若干个知识的一个分片，就像我们昨天发生什么事情，前天发生什么事情，就像这个左边这张图里展示的这样子，那么有了这个分区以后啊。

我们如何让这个模型根据这些记忆，去这个这个去完成这些任务呢，比如说我这个今天开了职员大会，大家听了这个报告，那这个报告里边有哪些信息呢，那模型该怎么知道这些新发生的这个信息呢。

我们这里提供了两个这个技术的一个思路，一个是直接对过去这个知识分区里边的，所有信息进行一个墨迹，进行一个合并操作，也就是说不管你昨天发生了什么，前天发生了什么模型，都不管372 11的把它给墨迹在一起。

然后共同的去作为一个外部的一个memory，然后这模型去啊得到这些信息啊，另外一个思路非常简单，另外一个思路就干脆直接检索，因为不同时间段发生的信息，可能是重要性是不一样的，所以我们也提供了一个知识。

继续检索的一个思路来去让这个模型去这个啊。

![](img/023b9ed505a39fe86fa1685eee1d0822_27.png)

对这个知识进行一个这个区分好，那么呃，另外一个就是比较关键的就是门控机制了，这个门控机制其实就快，决定了这个模型，什么时候使用它固有的这个记忆，那什么时候又会使用它这个外面经过编辑好的。

这个相关的这个记忆，那这里边我们其实设计了一个非常简单的一个，优化目标，来让模型啊相当于去学习到这个，如果这个知识啊相当，如果假如这个知识可能是跟这个啊，前两天发生的信息有关的。

那么可能就倾向于这个一个啊，或者是今天相相关的，可能倾向于这个旁边这个回路，如果是跟这个模型本身它自己会的知识相关的，那就倾向于走上面这条烂路，就会直接走这个绿色的这个链路。

这样的话也是尽可能去避免这个模型，去影响到他的这个performance好。

![](img/023b9ed505a39fe86fa1685eee1d0822_29.png)

那么它的效果怎么样呢，呃我们其实做了很多大量的这个实验啊，我们做了一个这个，相当于持续对进行大模型进行，编辑的一个实验啊，从编辑一条到编辑第二条，到一直到编辑到1000条。

相当于就从12341直编辑可以看到啊，其实啊编辑添千条以后，这个模型还没有崩掉，就是模型能够去啊，几乎保持很高的这个原有的这个performance，同时它的准确率也挺高的啊。

虽然说好像这个有些结果上没有，这个有一个baseline高，但是啊大部分情况下，它能够在这么经历过这么多次模型编辑以后，它还能够保持到一个相对更高的一个准确率，另外的话就是我们也去做了一些。

对幻觉数据上进行分析，就看下这个方法能不能去用来减轻这个幻觉啊，也是可以发现它能够去修改，比如说在接近600条的这个啊编辑以后，它还能够去保持一个比较低的一个困惑度，也就这个模型还没崩掉啊。

因为以前有很多工作，其实包括我们自己团队也做过啊，编辑久了以后，这个模型就崩掉了，就基本上不work了，这也是很大的一个问题，还能有一个问题，就是还能泛化到一些啊。

这个没有见过的一些自然语言的这个这个事例，也就是说它还有一定的这个泛化的这个情况，好啊，当然这个它也会带来一些这个代价啊，就是说会增加这个我们的这个计算成本，但相对还是在一个可控的一个范围内。

还会增加3%到四的一个计算。

![](img/023b9ed505a39fe86fa1685eee1d0822_31.png)

跟这个推理的一个这个成本，而我们也其实也发现它有些可以改进的地方，比如说这个检索的这个思路啊，其实检索它有个天花板，就是目前呢其实检索的方法，其实可能还，这个。

我们使用的是比较NA业务的一些检索的方法啊，也就是说他可能没有找到一些啊，记忆中比较重要的一个一个一个一个一个区域，所以说可能就检索的事，还有很大的这个提升的好，那以上其实都是围绕这种啊。

我们给模型去新增一个知识的一个一个一个，编辑的一种情况，那么我们有没有可能去啊，把模型中的某一部分知识给它擦除掉，或者是说让模型让语言模型去忘掉一些知识，这个场景其实对于这个啊搞大模型安全。



![](img/023b9ed505a39fe86fa1685eee1d0822_33.png)

其实非常有用的，因为我们都知道这个模型有些时候会吐出来，一些我们不想让它吐出来的话，包括一些这种安全的或者是隐私的一些信息，那我们也尝试了去如何用编辑技术来去做这种，大模型的这个去读来。

去让大模型变得更加这个安全的这个可信，那这个工作其实啊非常好理解啊，就是我们当时去探索这个语语言模型，假如是一个常规的语言模型啊，我们可以看到其实可以通过一些越狱的prompt，让它吐出来一些这种嗯。

有些时候会有一些这个非常敏感的一些内容啊，其实我们也试过像包括像DPO等等一些方法，甚至对齐的方法，其实对齐后的这个模型还是有可能会被这个啊，prompt给它攻击到的，就模型本身其实还是有这种情况的。

那么有没有可能有一个比较精准的方案，直接找到这个原模型中间的，跟这个有毒信息的这个区域，然后我们把这个区域的信息给它改掉，让它这个变得更加安全呢，那我们只是做了一个比较初步的一个探索呃。



![](img/023b9ed505a39fe86fa1685eee1d0822_35.png)

这个工作，因为其实这个这个场景没有现成的数据集，所以我们首先干了一个事情，就是先构建了一个数据集，这个数据集叫做safe editor啊，我们当时收集了一些已有的公开的学术，数据集啊。

啊这个其实这个领域还是有很多公开的数据集，还有公开的一些BROM的，我们就收集了很多数据集，然后去自己构建了一个啊新的数据集，那然后接着我们就去尝试了一些已有的方法。

并且设计了一个非常简单的一个这个基线啊，这个机线啊其实思路非常简单，是个非常simple的机线，就是我们先做一个定位，我们根据这个安全的表征输出，和不安全的表征输出，我们假设啊。

但这个假设也不一定非常严谨啊，就假设它之间如果是差距非常大的，那么可能这个区域就表征了这些，非常有毒的这个信息，然后接着就是一个思路，就是我们直接去修改这个区域的这个参数，让它实现这个模型的一个剧读。

当然去读的时候，我们也为了保证它的通用能力，也会去设置一些通用能力的这个数据的，这个这些正则项，来保证它通用能力不会掉的特别厉害好，那效果上其实也是非常不错的，就是虽然说还是有一些副作用的。

可以看到我们当时测的，在三个这种公开的performance数据啊，去测了一下，结果发现还是会掉一些点的，但是相对还是可以去啊，保证一些这个副作用的副作用的，这个相对比较小的控制范围内。

同时它也可以在呃相对其他的一些机械来说，也是能够有一定的程度的这个提升的，那么这个思路它背后和一些传统的一些做法，它到底有一些什么有些本质的区别呢，啊我们又去做了一个比较深度的一个分析，就是发现。

其实像传统直接去T或者是做对齐的方法，就像这个啊下边的这样图例展示，可能这个读的信息还在里边，但是它可能被绕过去了，就是模型遇到这些输入的时候，它可能一些有毒的信息，仅仅是绕过了这个一些信息而没有啊。

像然后然后吐出来一些信息都是相对安全的，但是这个呃编辑的做法呢，他直接把那部分信息给改掉了，相当于是他因为做了一些梯度的一些操作，所以它被信息被改掉了，所以相对而言，整个毒性的这个权重就被降低了啊。

但是这个客观来讲啊，其实也没有被彻底擦除掉，因为这个呃为了保证这个模型的通用能力，不受太大影响，我们只是呃略微的让它这个信息，相当于略微的为它降低它的这个权重，但是事实上还是存在的啊。



![](img/023b9ed505a39fe86fa1685eee1d0822_37.png)

但这个只是一个初步的一个探索，那这里是我们当时做这个编辑，做了一个比较简单的一个demo啊，这个在我们的这个呃这个代码也是开源的，如果呃有感兴趣的可以去尝试一下。

就是我们可以看到这里边就是对于给定的输入，跟这个输出的一个不安全的回复，和一个安全的回复，我们去首先做了一个这个相关的一个定位啊，可以看到这个它很快就可以定位到，这个这个层的这个这个区域啊。

大概是这个是在32层啊，然后后面的话就是进行一个这个，编辑的一个操作啊，因为时间有限，边际操作我就后面就不展示了，这个编辑，它可以成功的去让这个模型去突出些啊，相对啊这个这只是这个结果。



![](img/023b9ed505a39fe86fa1685eee1d0822_39.png)

就相对于把一些原来的不安全的输出，变成一个安全的这个输出，好啊，我们把刚才所讲的这些工具跟方法，都集成在我们的一个团队的一个开源工具里，叫做这个easy editor啊。

它目前其实也支持我们很多的国产的这个模型，包括像这种百川通义啊，啊GLM下了GLM啊等等。

![](img/023b9ed505a39fe86fa1685eee1d0822_41.png)

也包括一些国外的一些这种主流的一些，这种模型等等，好，那最后做一个简要的这个总结与展望啊，其实总结和展望主要介绍的信息，都是这个我们现有工作的一些，这种局限性跟limitation啊。

虽然看到其实刚才我所讲的这个编辑新增知识，擦除知识啊，看上去效果还行，但实际上它还是有很多问题的啊，就比如说啊，我们如果在1000条之后再继续进行编辑的话，这个模型其实还是有可能会崩掉，也就是说。

其实目前啊这个我们当时做了一个实验，就分析，假如说把一条知识啊，比如说以例子，比如说就以这个美国总统这个例子为例啊，假如说这个美国总统从这个A换成B，然后我们如果有一天这个这个这个B又变成A。

就相当总统又变成A了，那这个这个知识，这个模型还是不是原来那个模型呢，其实我们从这个左边这张图里比较，其实可以发现啊，参数化的这种知识的一个更新编辑，跟以前的像符号主义的这种知识图谱的啊。

知识的更新机理是完全不太一样的，就是很可能模型通过一些其他的机制，比如说下面红色的这个神经元，来实现了这个新知识的一个表达，但是这个蓝色神经元这个还在，就这里边说明一个很难的问题。

说明我们其实对这个知识的机理，了解的还是比较浅的啊。

![](img/023b9ed505a39fe86fa1685eee1d0822_43.png)

哪怕用这个回路的思想来去解释，其实里边还有很多很多的这种问题啊，就背后带来了一个问题，就是现在目前其实还很难去实现，一种比较精准的终身的这个知识的编辑，但是呢其实这方面还是有很多。

我觉得比较有趣的一个方向的，比如说最近有一个工作叫做表征的这个编辑，或者表征工程啊，其实我们如果假设把这个参数化的知识，其实都用这种表征来去理解的话，其实我非常看好这个思路啊。

其实里边有很多很多有趣的现象可以发生，我们可以通过去编辑它的表征去控制它，让它这个变得更加安全啊，常常掌握一些新知识，让他尽量避免幻觉啊等等，这都是我觉得是一个非常有趣的一个这种视角。



![](img/023b9ed505a39fe86fa1685eee1d0822_45.png)

但是整体而言啊，其实我们现在啊对这个方面的研究，有点像以前的这种物理学的研究一样，就是现在其实这个大黑盒，而且参数量很大，我没有办法去精确到每一个神经元，去分析它到底是什么样，很多的时候都是做一堆实验。

外部干预的实验，比如说我们就设计一些problem的，设计一些探针，或者设计一些啊机理分析的一些，这种可视化的一些角度等，去分析这个到底是什么，但这里边其实很难去非常完备的去理解，它到底背后是什么啊。

其实右边这张图是我最近看到一篇工作，他非常有趣啊，一篇ICMN工作，他把大模型的机理，跟人脑的机理来进行了一个类比，也就说明其实我们大模型很多分析现在有点像，跟人脑的分析越来越像了。



![](img/023b9ed505a39fe86fa1685eee1d0822_47.png)

这背后有很多问题，其实都是有一些共性的这种问题的，那最后一页啊就是呃，当然这个其实也是这个一个一个一个，一个DISCANNER，就是我今天其实讲的东西呢，其实很多都是一些假说啊。

因为啊像刚才这个贺老师也提到，现在有很多新的架构，比如说MUA，它背后的这个机理是不是和transformer一样的，很难讲，不一定一样，也可能一样的啊，最近有一篇工作，提出了一个叫柏拉图的表征的假说。

他就说可能不同的架构啊，甚至不同的模态的模型，最后都是在去逼近一个可能是一个世界模型，但这个给我们带来一个很好的希望啊，就是可能假可能是一样的，但是未来还需要很多呃同行去探索啊。

如何去建立一个非常完备的知识存储，表达更新的一个理论体系。

![](img/023b9ed505a39fe86fa1685eee1d0822_49.png)

好以上就是我的报告。