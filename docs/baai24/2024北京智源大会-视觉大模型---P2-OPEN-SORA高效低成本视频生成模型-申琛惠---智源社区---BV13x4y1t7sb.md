# 2024北京智源大会-视觉大模型 - P2：OPEN-SORA高效低成本视频生成模型：申琛惠 - 智源社区 - BV13x4y1t7sb

各位朋友大家好呃，我今天非常荣幸可以来到这里呃，为大家分享我们open sora的这个模型，嗯这个幻灯片，它右侧展示的呢是我们第一个开源版本，也是今年3月刚刚公开嗯，所展示的生成的这个海滩。

然后嗯我们希望可以通过这个模型呢，让更多的中小企业可以以比较低的成本，参与到这个视频生成模型这个嗯当中，那嗯open sa这个项目呢，是在由洋教授的指导下进行的，呃，嗯就是这张照片是有阳教授。

在伯克利答辩的时候拍的，然后他身边是几位伯克利，在高性能计算和视觉领域的教授，那可以说这两个领域呢，对我们open sa的项目都是有非常密切的关系，呃先简单讲一下我们今天的大纲。

就是我会首先简单的介绍一下SORA，然后为为什么我们需要嗯，想要去做这个open sara的项目，然后接下来是对open sora的一些技术解析，然后当然就是要嗯做到低成本高效率的训练呢。

嗯当中也用到了路程科技，这个CLOSALAI的一项加速工作，然后最后呢我们会嗯给大家展示一下，我们最新版本，也就是其实是下周即将公开的这个V1。2，版本的的这个推理资源的测速。

然后最后是嗯与大家分享一下我们的模型效果，在这边呢嗯也是主要是想嗯抛砖引玉，因为我们认为就是说在视频生成哦，目前其实还是处于一个非常早期的阶段，就是说虽然市场上，我们已经有了一些商业化的产品。

但是要做到一个非常成熟的，这个嗯生成的模型还是有一段距离，目前可能还是离不开大量的剪辑和编辑工作，那嗯希望嗯通过我们这个open sora的，这个嗯初步探索的成果呢，就是嗯能够激发大家兴趣。

然后进行深入的讨论，那首先我简单介绍一下SARA，我相信在座的各位嗯，对open aid sora都已经非常的熟悉了，就是他是一个OpenAI开发的，生成式文本到视频生成的模型，然后嗯其实在他之前的话。

市场上呢，就是嗯已经市场上已经有了一些呃模型，像比如说软尾m l p custable video，但是在SARA出现以后，就是他从这个视频的生成时长，甚至质量来说都已经远超嗯当时的模型。

然后达到非常令人惊艳的效果，嗯然后嗯大视频这个嗯模型生成这个呢，其实嗯它的应用前景也是非常的广泛，就包括到游戏艺术媒体制作药物研发广告，教育各种嗯这个行业，然后我们可以看这个幻灯片。

右边这张图呢是扎克库科夫，他在twitter上面的一则发言，就是说，嗯视频模型使我们以嗯低成本制作这个电影，也是成为一种可能，那嗯虽然说嗯open sa它的效果非常的好。

但是嗯就是OpenAI的SORA，不好意思，它效果非常的好，但是SORA目前是没有公开的版本的，一方面呢可能说明这个嗯SORA，他这个大视频的技术并没有完全的成熟。

另一方面可能也是说他的使用的消耗的成本，是非常的高昂的，那即使它的功能非常的强大呢，它封闭的特性也导致说呃，目前我们不能对它的应用场景，或者这个模型进行进一步的用途的拓展。

因此呢我们就开发了open sora这个模型呃，首先需要声明一点，就是open sara跟OpenAI的SORA，它是完全不同的两个模型，然后我们做的是嗯，根据这个OpenAI sora。

它的技术报告中的一些嗯技术，来做了一个类SA的模型，然后希望呢也可以帮助大家更好的理解，如何去复制这样的模型，或者或者就是说在自己自己的产品中，可以体验类似于SARA这样模型的效果功能。

open sora呢是首个开源的类SORA视频生成模型，然后我们主要目标就是说用低成本，完全开源的方案把它的模型引入社区，那我们公开了模型结构，已经训练好的模型参数嗯，完整的训练流程。

我们的数据预处理流程，也提供了一些视频生成的教程，方便大家可以直接使用我们的模型进行体验，接下来我来简单的讲解一下，我们这个open sora第一个版本中用到一些技术嗯，分为以下三个框架。

就首先我会分析一下模型的架构设计，以及我们的模型的背后的原理，然后在第二就是来讲解一下，我们的LISA的训练方案，第三也是非常重要的一个部分，就是数据的预处理，因为数据对于视频模型的训练来说。

是非常的重要的，那首先模型架构我们也是用了基于DIT的架构，当我们因为需要降低这个模型训练的成本，所以我们首个版本直接用到了pixel alpha，作为模型的初始化，那大家可以看这个幻灯片。

左边的这张图是我们第一个版本的模型架构嗯，可以看到我们嗯除了这个，因为pixel alpha它是一个纹身图的模型，但是我们在这个special attention，就是空间的这个自助力。

后面呢多加了一层时间的自注意力机制，通过完全将空间和时间嗯进行分开处理呢，我可以大幅度的降低这个模型的成本，那嗯右边的这张图是我们对于两个嗯，对于di it和我们提出呃，和这个我们的SDDIT的架构。

进行了一些测速，可以看到就是当这个token数量增加呢，就是这个STDIT，它在吞吐量上是有非常大的优势哦，那接下来呢呃会讲一下open sa的应用原理，嗯相信大家都看过SORA的这个技术报告。

那其实我们的原理呢嗯大家应该也不会陌生，就是我们会对视频和这个文本的控制信息，都会进行一个encoder进行编码，然后呢把这些编码的信息传入到STDIT中，嗯进行训练，这是我们的训练阶段。

然后推理阶段呢，就是嗯和diffusion model就非常相似，我们就是从这个嗯编码器的这个潜在空间中，进行随机的采样，然后嗯再把这个采样到的噪声，输入到STDIT中，让它进行降噪，然后嗯制造以后呢。

这个特征再经过我们的VAE的这个解码器，来生成我们最终的这个视频嗯，嗯接下来是训练细节的部分，嗯其实嗯就是open s呃，就是SORA的这个方案呢，他们的训练成本可能推测是在数千万到，数亿美元。

这是这个，使得其实大部分企业都没有办法参与到这个视，嗯视频的模型开发中，那我们的目标呢是说，要将成本控制在1万美元左右，嗯我们其实这个训练呢主要分为三个阶段吧，就第一个阶段就是说大规模的图像预训练。

第二个阶段呢是说我们进行视频的预训练，但这个阶段就是说我们的视频，它的分辨率会更低一些，然后可能它的质量也没有那么好，就让模型简单的具有一个嗯视频对视频的理解，然后第三阶段才是在进行高质量的那个视频。

数据进行微调，让模型在视频上面的表现进行显著提升，嗯其实第一个阶段的话，我们是之前也已经提到过，直接把这个已有的纹身图模型作为初始化，这样可以大大降低我们这个模型训练的成本。

然后当然也是嗯目前也是有很多不少新的工作，有不少新的这个纹身图的模型，大家如果想要做这个LSORA的开源方案的话，也是可以直接拿过来用的，然后当时呢是嗯我们看完第一个版本的时候。

并没有可用的高质量时空的VIE，所以我们就是把stable diffusion，这个嗯空间上面的VIE能拿来直接进行使用，这个也帮助我们降低成本，然后第二个阶段呢就是我们在这个阶段。

大概嗯第一个版本是经过了2800多个嗯，H800的GPU小时的训练，然后成本大概在7000美金的样子，主要就是说嗯我们的SDDIT的架构，就是引入了时间注意力的模块，那可以说就是纹身图模型呢。

我们嗯之前已经有了这个special self attention，他的能力假设哦，我们可以把它看成是一个初中生的水平，那我们新引入的这个temporal of attention。

其实嗯是需要完全从头开始训练的，它类似是一个婴儿的水平，那我们目标就是说在这个一起进行训练以后呢，让两者都可以达到大学生的水平，然后嗯其实我们在这个时候，就是直接进行了大规模这个数据的混讯。

然后就是说我们发现这样子模型，学习的速度还是非常的快的，然后由此也可以看出，就是说这个嗯这种大模型，它在学习上面就是尽管只是说它不同的框架，这个能力是不太对等，但是也可以进行快速的学习。

达到水平的快速增长，当我们也用了多样化的数据训练，来增强这个模型的泛化能力，稍后讲数据的时候，我也会分享我们用到的数据集，然后嗯我们对于这个不同的分辨率呢，也进行了优化。

以达到就是说我们在有限的计算资源，可以这个情况下，可以更加有效的训练我们的视频模型，阶段三呢，就是说我们用这个更高质的质量的视频数据，进行微调，这边呢我们也用了将近2000个GPU小时的样子。

然后这一步是跟第二部的最主要的区别，就是说我们这个视频的质量，它的这个像素以及时间都会嗯更更高更长，然后嗯在这一步呢，我们就可以嗯达到这个视频从短到长，从低分辨率到高分辨率。

然后从低保真到高保真度的这个视频的学习，Now on，其实我们借鉴于ul two这个技术，这个也是自然语言中嗯一个技术，就是说嗯把这个transformer的训练呢，也是灵活的运用了不同的掩码策略。

其中嗯可以说嗯对比如说前K帧末末尾的K帧，或者其中任意的K帧进行掩码来训练，我们的模型，这样子呢，也使得说我们的模型可以嗯，非常灵活的应用到不同的场景，包括就是说嗯基于一个图像来生成一段视频。

或者生成一个循环视频，嗯或者视频到视频的生成，以及就是说一些视频方面的剪辑的性能，然后我们用了一个五元组，就是你可以看这个最下方这个示例，就是我们五元组来定义，就是不同的应用场景。

就是使得我们这个在推理阶段，有非常高的灵活度，嗯那这边呢嗯我也再来讲一下，就说我们如何去支持这个不同分和高宽比的，这个视频视频训练，因为SORA他的那个技术报告中提到。

就是说用原视频的这个分辨率宽高比来训练呢，可以非常有效的嗯改善这个画面构图，因此呢我们其实用了一种嗯，分统训练的这个策略，其中就是分桶，主要是嗯根据这三个看，大家看左边这边resolution。

Non frame，as sporal就是分辨率，然后视频的帧数以及它的高宽比，来将视频进行哦一定概率的分类，然后我们提出两个参数，就是说keep probability嗯，这些视频它都有一定的概率。

用它的原本的这个分辨率宽高比来进行训练，然后同时batch size呢也有助于我们更好的平衡，就是不同这个resolution他的这个训练时间，来更好的利用我们这个GPU进行训练嗯，第三嗯。

接下来第三个板块，我要讲的就是我们的数据处理嗯，这边是我们open sara嗯，这个第一个版本所用到的一些数据集，然后嗯其实我们的数据总量是非常的大，大概在100tb的样子，这对于我们的存储而言。

其实是一个非常大的挑战，嗯我来简单分享一下我们这个数据的收集流程，大家可以看，从图的最左边就是我们起始点是一个视频，一个video，然后我们会对这个视频的场景呢进行检测，把它分割成不同的这个短的视频。

然后我们会它对它进行美学打分，就比如说你看这个esthetic score，4。5或者6。5，以及我们会对它进行这个光流分数的打分，以及视频中这个嗯有的这个文，文本信息进行检测，就当这些处理。

就是我们都觉得它的质量令人满意之后呢，我们才会进行下一步，也就是说对于这个视频描述进行标注，然后当有了标注呢，我们也会再进一步去看，就是我们这个视频描述的这个标注的内容，是否和视频嗯近有一个有效的对齐。

然后在之后我们也会去检测这个镜头的移动嗯，以及一些嗯，这就是那种各类各类的这个嗯，我们数据清理的流程，然后以达到说我们的训练数据嗯，它是具有高美学分数嗯，大的近镜头运动以及这个强的语义一致性。

这样子的特点嗯，来训练出更好质量的模型，那这个对视频的这个描述的标注呢，其实open呃，其实SORA就是a OpenAI的SORA呢，他们是用到了g p t four v，这个模型来进行生成。

但是嗯如果我们也用这个的话，这个成本肯定是非常的高昂的，所以我们用的是一个开源的lava1。6，来进行一个自动标注，然后嗯后续的话我们也是通过这个match sc，这个分数，来确保说。

我们这个生成视频描述的质量是过关的，然后接下来嗯，我来分享一些我们这个数据上面遇到的挑战，嗯其实最典型的就是这四个，就是一个是高节点负担，据说我们训练的数据，可能单个数据集里面都是上达上万条嗯。

简短的这个视频文件嗯，小文件，但是数量非常的大，就会造成这个高节点的负担，然后此外呢就是是不断增高的嗯需求，因为我们嗯在也在快速的进行这个模型的迭代，目前呢我们已经开源了两个版本，然后也是将在下周开源。

我们的第三个也就是version1。2的版本，其实我们发现说我们这个数据的规模，在以每个月50TP的级别进行增长，然后第三点就是高性能需求，就是我们训练模型呢需要非常快速的去读取，这个大量的视频数据。

所以我们需要这个低延迟的性能，同时呢我们每训练一定的阶段，都会对这个模型的checkpoint进行一个存储，这个写入存储呢也是需要高带宽样子的性能，然后最后一点就是高切换成本。

就是假设说嗯我们在这个我们的存储，如果想要在多云或者多肌群训练场景下，进行数据同步或者数据迁移的话，因为我们这个数据数据量是巨大的，所以我们的时间成本也是非常的高，嗯那其实嗯介绍完整体的模型算法。

这个整个的流程以后呢，就是我们要考虑如何把这个成本降到最低，那嗯其实我们如果说要去租借一台H800呢，就是每个月的成本，可能会达到8万到10万人民币，那假设如果用到吧台呢，可能租金就要高达80万。

所以我们如果想要一次性的，把试验成本尽可能的减小呢，其实我们第一个版本只用到了八台，H800是64个，这个GPU进行了非常短的这个嗯训练时间，就是把速度和效率提到最高，然后我们的这个低成本的训练。

其实离不开classical AI的这个加速的，close AI呢，也是嗯，路程呢团队在高性能计算领域的一项优秀工作，它是一个专门为嗯大规模的AI模型的训，练和推理而进行深嗯设计的一个深度学习系统。

它的目标就是说我们要最大化这个计算效率，而且最小化部署成本，就是说嗯再不用大幅度改动，就是已有的代码，其实我们只需要进行嗯几行的代码的改变，就可以很便捷的运用到这个系统，然后他现在嗯大家可以看右边。

就是支持这个不同的平台嗯，那它主要是分成这样子的三层，第一层就是高效内存系统，第二层是N维的并行系统，第三层就是这个低延迟的推理系统，这样我们就可以最小化我们的部署成本，好我们先嗯来看一下。

就是第一层的结构，其实我们用到的是一个异构内存的这个系统，那嗯简单的来说就是说当模型特别大的时候呢，因为我们这个GPU内存空间非常的有限，所以我们可以灵活的利用到CPU或者硬盘中的空。

间来进行实时的这个数据交换，然后让这个单卡训练比较大的模型，也成为一种可能，然后呃我们的高效训练策略呢，其中也用到了非常多的并行策略，就比如说前面的这个左边流水线并行，多维张量并行嗯。

都是模型并行的一些方法，当然我们的这个多维张量其实也是有，就是对于数据的这个激活的一些优化的方法，然后最后面的序列并行呢，其实嗯这也是这个视频训练的一个难点吧，就是说嗯常见的一个困难。

就是我们视频随着这个分辨率的增加，以及训练时长的增加，他的token数量可以达到百万级别，那就是要如何兼容这个非常长的序列嗯，序列在模型上进行训练呢，可能就需要用到这个序列并行的策略嗯，诶不好意思。

当然就是这些嗯各种变形策略，就是嗯大家都不用去担心，因为其实这个CLOSALAI的系统已经它在内部，就是把这些优化都已经做好了，所以研究人员其实只需要去关注这个模型，它的设计和训练就可以。

就不用关注这些分布式计算的内容，嗯那这边是我们open sora在close AI上面的加速效果，就close AI，其实我们嗯这边左边的图呢，其实对我们的文本编码器和这个视频的编码器。

也是进行了一些加速，提升了它的吞吐量，然后也把TFI所占用的计算资源呢进行了嗯，这个份额让它显著的下降，我们可以在这个64帧，五百十二的分辨率视频的训练给它达到加速嗯，55%。

然后在单台服务器上面也可以训练，长达一分钟的高清视频，然后这个提速也达到了16%，接下来嗯想要给大家分享一下，我们最新的这个V1。2版本的嗯，推理资源的测速嗯，这边是我们的一点一比领大小的模型。

然后如果大家看右边的这个表格，就可以看到说这个1080P高清视频，如果我们想要生成16秒的视频的话，其实这个我们的token数量就已经达到了一个，mil领，就是百万级别的这个长度。

而且这个其实已经是基于我们的via e，对于时间上已经进行压缩了，就是如果我们没有压缩的话，这个序列还会更长，然后左边呢是我们这个在单台机器上面进行推，算的一个延迟，其实说这个时间还是非常的惊人的。

如果大家看这个八秒嗯，最高清的这个视频生成，它甚至可以达到800秒，而这种非常长的延迟呢，也是为这个视频投入应用带来了非常大的挑战，但是也可以看出就是说嗯我们在相同的分辨率，这个视频生成长度的增加。

和这个它实际生成所需要的时间嗯，我们自己的测试发现，它其实是一个线性的关系啊，嗯然后这边是我们这个对于存储需求的测速，测速呃，这个存储需求这个测验的效果，然后接下来这边呢是我们嗯三比零大小的。

模型的这个测试效果，左图是它的这个推理就是时间的延迟，而右图是他对这个存储的消耗，当然就是说随着模型的增大呢，就是如果我们用这个单单的一个H800的GPU嗯，需要注意一下。

这边其实我们测试只用到了一个H800的GPU嗯，它其实是没有办法容纳，就是我们特别高清的长时间的生成的，这也就是说嗯我们就是在这个视频生成，用到多卡并行进行生成部署是非常的重要的。

嗯那嗯有了之前的这些观察呢，其实嗯我们这边如果就是做出一个假设，就是说我们这个视频生成的所需要花费的时间，假设与我们这个视频本身的时间长度，它的这个space也就是他的这个视频的像素。

以及这个我们模型的参数量大小，假设它是呈线性的关系，也就是说我们之前所观测观察到的这些现象呢，那嗯就是说如果我们做出这个假设，如果OpenAI的SORA它是一个30比脸大小的模型。

如果他想要生成一分钟的高清视频，那它可能需要12个小时，但是根据现在就是网上一些信息说，它生成一分钟的视频，大概需要一小时到两小时之间呢，那我们推测说它SORA这个模型参数量。

很可能是在三比领到七比零之间，但嗯也是在这个假设成立的前提下，好就是，那接下来呢我展示一下我们的demo呃，我们的demo其实是跟这个OpenAI是没有办法去比的，那主要是以下两点原因。

就说我们是在一个低成本的条件下完成的训练，open air呢，它使用到了大概2000到4000个H100的GPU嗯，花费了可能将近5000万美元，到2亿美元的这个训练成本。

而我们其实用了只是不到1万美嗯，大概1万美金这样子的一个，嗯范围的成本进行试验，所以在这样非常有效成本下呢，就是我觉得我们取得的效果是可以接受的，而且通常说这个训练数据它的质量越高。

那生成的视频呢质量也会非常的好，嗯我们之前采用的分阶段的训练呢，其实也是一个降低成本的一个方法，就是说先用一些嗯质量没有那么高，可能甚至就是这个像素也更低的，这个数据进行训练，然后最后再进行高清。

来拉高他的这个视频生成效果，但这一套方法其实在这个自然语言大模，那个模型训练领域呢也是已经实行多年了，就比如说当年这个BT的训练也是分为两个阶段，先用128个token进行生成。

然后后面再把它提升到五百十二个token来，这样增强它这个模型的生成效果，OK就是说嗯，我们目前呢这个最新版本就是可以生成，单次生成大概是20秒的视频，那其实我们针对于我们最初版本，这个只能生成两秒了。

有了显著的提升，然后用到说，如果我们视频基于嗯之前视频生成的，这个延续性生成的可以长达数分钟，所以嗯OK，那我们先接下来直接看一下这个演示吧，这个是我们嗯已经公开的最新版本的，这个不同比例的生成。

然后我们可以生成较为高的分辨率，当时是可以生成720P，但这个是我们之后嗯，下周将要开源的这个模型的一些生成效果，然后我们嗯之前的模型也可以做视频编辑，然后可以基于图像来生成视频。

然后当然也是可以直接进行图像的生成，那嗯其实我们在GITHUB上面呢，就是获得新标数量的增长也非常快，自从我们今年3月开园到目前呢，只是已经获得了大概1。10000新标用户呢，群体遍布全球。

然后这个是我们市社区的一些使用案例，是我们最初的版本的一些效果，虽然只有短短的两秒，其实嗯我觉得当时在这个低成本的前提下，这效果还是不错的，然后这个也是这个社区用户这个一个使用案例。

就说只是经过一些简单简单的这个文本提示，也可以达到一定的生成效果，那我们的未来计划就说首先在模型方面嗯，我们肯定是进一步想要增加模型的大小，来提高生存质量，然后开发这个时域的变分自编码器。

就是我们下周这个版本应该会直接公开，这个已经嗯最新的版本投入使用，然后接下来就是说我们的数据呢，因为我们就是之前几个版本，它在人像上面的生成质量，只是嗯不是非常的好，因为主要也是因为这方面数据的欠缺。

所以目标是收集更多像这人像数据，来完善这个生成人类相关内容的这个能力，然后会进行更精确的美学评分，或者嗯也是进进一步的优化，来加强它这个视频的生成的强度，以及它的质量效果，那非常感谢大家的聆听。

嗯也欢迎大家在我们的GITHUB上面自由探索，就是目前呢我们也在招聘实习生和全职人员，如果大家对这个项目或者路程感兴趣呢，也欢迎加入大嗯加入我们，祝大家有快乐的一天，谢谢好。

非常感谢那个我们再回答几个问题可以吗，非常感谢那个陈慧，那么呃我觉得我觉得就是说呃，哦像开源的这种project的话，跟公司project其实一个最大的一个差别，就是说所有的细节都可以分享，对不对是吧。

然后就是如果是企业界的话，因为呃其他的因素考虑的话，一般有些东西是没有办法去分享的，所以在这的话我大概给两个问题的机会吧，对诶给那给成话筒，最左边那个，喂您好，想向您请教一下，就是在做的过程之中哈。

呃就是我们对于呃不同的图像，因为刚才最后您有提到说，增强人像这块的主要能力，那这个其实就是代表的是，我们对于图像内容的不一样，导致我们对于图像最后所形成出来的生成结果，是有不同差异的。

那么我们是不是有做过，针对于不同类型的图像去做一个评估，就是从什么样的情况下，我们需要什么样的资源，在什么样的架构的情况下，我是可以去启动起这样的一个，围绕着这种图像内容的一个四。

skin in law的一个起点的，呃，在这个没有没有来之前，比如说我55G的人像数据没有来之前，我这个实际上是没有spin了，LOD一点一点意义或者是概念，那这个实际上其实对于我们再去准备一些专业。

垂直领域里头的一些这个这个图像，或者是这个这个视视视频的这个这个呃，这个生成上面可能会有一些指导意义，所以我就想请教一下，说这个事情在我们open sora在去做这个过程之中。

有没有针对于具体的某些内容，然后对它去进行一些这种呃资源计算的，一个是稍微简单一点，嗯对好嗯嗯好，非常感谢，就是您的问题是说我们的在数据处理方面，有没有就是对他的不同的这个数据的嗯内容，来进行分类。

来进行这样一个处理是吗，对于不同的内容进行呃分类，然后基于这个分类来去进行资源，使用水平的评估啊，嗯其实我们如果您说到这个资源使用水平的话，嗯差别最大的，其实他的意思可能是不是说，就你把数据分成几堆。

就是每一堆的话就是可能用的就是用特用，就是就是相当于是数据的SAMPRING的ratio，是不有有一个这样的一个处理，其实数据sampling ratio呢，就是呃我之前诶可以再分享一下。

我之前的slides吗，我们有一个这个数据的这个bucketing，configuration的一个嗯，或者我就简单讲一下，就是说其实还没做是吧，就直接拿着那个，就就直接拿这个数据就直接用了，对不对。

我们对，就是说我们嗯也做一下简单的处理，就是说这个根据他的这个不同的resolution，然后会让他有一定的概率用这个，当然就是说我们用更高的resolution的时候。

我们也会耗费就是更加高的计算资源，所以在这一方面呢，我们是也是进行了一些测速，就是说确定说我们哦每一个不同的resolution，我们会用到不同的batch size来说。

我们训练不同的resolution，它可以达到一个计算资源的平衡，然后嗯在对数据质量的评估呢，就是我们其实现在是一个非常笼统的一套流程，就是进行了这个美学分数啊，就是光流分数的这一部分的统计对。

因为时间问题的话，我我我就我就不啊等，因为等下还有pa环节还可以问，但我觉得有个问题可能会带来比较感兴趣，就是说啊，因为你前面的模型train的时候，是用2D v e train的嘛。

但是你现在这个V1。2的时候，你是用那个就是啊相当于3DVAE做的，那么这时候的话，你其实你这个continue training就是以前的checkpoint的话，他就转过来时候的话。

其实他的那个就是latent space，它就变了嘛，你这个地方在做continue training的时候，有一些什么trick没有，这个嗯这个其实嗯我来讲一下，我们最新版本的这个嗯VAE的训练呢。

就是直接做了一个VAE的adaptation，然后这个方法其实我们是受这个pixel sigma，这篇论文的启发，他当中就是也是做了一些实验，然后发现说这个嗯diffusion模型。

它对这个VAE的adaptation其实是非常快，然后适应性非常好的，当然就是说先把把那个那个模型的参数那边先，比如说有一些地方先fix主先做一，做一段时间adaptation是吧。

就是说其实我们比如说我们已有这个模型对，然后我们新训练这个VAE，那我们可以把这个VAE拿过去以后，做一个这个模型和VAE在一起的混讯，然后让他做达到一些adaptation。

就是适应这个VAE的这个嗯，这个嗯他的这个效果OK对，然后嗯就是说因为目前嗯，其实我们也没有太多的这个计算资源，去做很多的试验，所以我们的这个策略，其实也是基于说对于一些论文。

他提出的方法就觉得可能不错，然后就直接投入到我们的训练当中，OK这可能是一个非常关键的一个细节，好那我们再次感谢春慧。

