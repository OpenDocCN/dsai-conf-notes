# 2024北京智源大会-多模态模型 - P6：圆桌讨论- Multimodal Perception and Generation - 智源社区 - BV1sT421a7FU

好大家好，今天很高兴有这个机会，就是精心准备了一个圆桌的环节，就是我们刚刚也听到了很多我们多姆太，包括视觉语言最新的一些进展，然后整个领域的进展也非常快，我们也想借这个机会。

然后有几位非常资深活跃的学者，然后我们讨论一些现在领域内比较核心的一些问题，然后当然也包括技术的，或者包括一些这个高级的问题，然后刚刚在座的三位老师，大家刚刚都已经听过他们的精彩的报告了。

然后线上我们还有两位嘉宾，然后我不确定已经，还没有，对对对我先介绍一下，对这个是佳辉，然后佳辉之前做过很多非常突破性的工作，包括这个party等等。

然后他现在是OpenEye的perception team的负责人，然后今天也是很荣幸有佳辉加入我们，来一起讨论一些问题，然后待会也很期待听听佳辉的一些见解，佳辉能听到吗，感谢邀请能听到，好的好的。

然后另外还有沈春华老师，沈老师，大家好，大家好，我很不好意思，我这个笔记本倒了半杯水，这个摄像头坏掉了，沈老师大家我相信也比较了解，他是国内比较资深的学者，也是我自己的这个导师，然后好。

那我们今天就直接开始，然后我准备了一些问题，然后不一定能问完，最后希望给大家一些提问的机会，然后对我们就直接开始，然后第一个问题，我觉得也是开场一个比较好的一个问题。

是大家觉得就我们看到视觉跟语言的发展，先是image ad的带领了我们很多年的发展到语言，那在现在这个时间点，大家觉得视觉是AGI的核心部分吗，然后有可能以什么形式贡献。

并且能把我们往AGI的路上带到哪，然后，看哪位老师先，只有一个话筒吧，都有都有都有，要不谢老师，我能听到吗，应该能看出来，我的回答肯定是，我当然觉得Vision会是，通往AGI路上的一个。

一个critical path中的很重要的一部分，至于为什么是这样，我想说可能，想问在座各位你愿不愿意，丢弃你的Vision对吧，我觉得，我觉得可能应该，大家可能并不愿意做到这件事情，我觉得是这样就是。

另外一件事情是AGI我觉得，跟很多，它不仅仅是智能，它对人类的经验，情绪等等这些，非常，很难去通过语言去捕捉的东西，也是紧密相连的，然后我觉得，Vision作为一个我们很重要的一个。

sensory的medium，我觉得是必不可少，但可能之后可以继续讨论一下，跟技术问题，但是至少我这边的观点是，我觉得Vision是必不可少，然后我也觉得。

大家应该work on Vision more，好谢谢老师，我的观点也是，Vision是必不可少的，但可能因为今天，在座的几位，都是做Vision Perception的。

所以我估计可能大家观点都差不多，对，我觉得从一个角度来说，就是那个，其实就是说，分析人的这个认知的话，就说有一类叫做是，Explicit Memory，就是显示的记忆，那部分就是说。

你的那些记到的一些东西，学到的东西是可以用语言来进行描述的，这个是Explicit Memory，然后其实还有很大的一块，叫做Implicit Memory，里面其中有一部分叫做。

Procedural Memory，就是过程记忆，或者就是隐世记忆下面的过程记忆，比如说你学会怎么游泳，怎么骑自行车，怎么投投篮，比如说用One Motion，一段式投三分。

我最近在练One Motion，我练了快两年，最近总算是练好了，然后结果刚打了一两场，学校的教职工篮球赛，结果就受伤了，然后打不了了就，对 然后，就是这些东西的话。

就是这种Procedural Memory，为代表的这些东西，其实它不是用语言，可以很精确地进行描述的，它是你的一个Experience，它是你需要，就像刚才谢在霖老师说的。

这个Sensory Grounding，它是通过这种方式去进行学习的，所以我，Anyway 我觉得视觉是非常重要的，我说完了，我觉得前面几位老师讲的都已经，基本都讲到了，然后，包括我自己的Talk里面。

我也提到就是，我相信Real World Experience，是需要Vision的，如果我退一步讲就是，之前唯一没有提到的是一个可以，较为哲学的观点是什么，就是我们到底为什么要Vision。

生物体为什么要Vision，那么你可以想象，比如说曾经的哲学家，比如说亚里士多德他说，我们要知道往哪看，这个特别地，有点形而上学的感觉，但是你要如果想从进化的角度来说，那就是纯从达尔文的角度来看。

那我们为什么要Vision呢，Well 你要繁衍生息，就是Make babies that can make babies，that can make babies forever。

但是这个好像也不是很，就是特别地，令人满足这个答案，我觉得很多时候是一个，一个不只是一个这种，这种竞争的关系，或者生物体之间的竞争关系，还有一个就是一种，从真实世界中学习，和交互的一个过程，然后我觉得。

从这个角度来看，Vision应该是在，智能形成的过程中必不可少的，江辉有什么补充吗，我很同意大家的观点，就是说我觉得，多莫泰是AGI的必不可少的部分，然后当然就是说，理由也跟范云说的有点像。

就是说我们怎么来看AGI，定义是什么，那有一种定义就是说，AGI是希望能够，就是说，完成大量人类能完成的，具有经济价值的，合法的一些任务，那我们可以想象这个事情，就是说假设明天一起来。

我们就是不看任何东西，你可能本来一天能完成20个任务，你看看就是说，我们还能完成多少个任务对吧，就是所以这个是，就是说非常显然的一个答案，当然就是说，就是说大家都说这个，这个是需要的。

就是说我觉得有一种情况下，有可能是不需要的，但是这个也跟AGI的定义有关，就是说我们今天讲AGI，我们还是希望它能够，benefit human，就是不是简单的一个智能体，但就是说自己跟自己。

我觉得有一种情况下可以想象，就是说如果世界上都是这个，规矩的智能，就是说没有人类的话，有可能它是不需要一些感知，或者说视觉，或者说某种这个传感器，但那个就是说，跟我们今天所定义的AGI所不一样。

所以回过来就是说，AGI是要服务人类的，那它是需要这个感知，它需要这个多模态啊这些，好谢谢嘉惠，施老师能听到吗，能听到能听到，对那个我非常同意，刚才嘉惠的观点，就是说这个，视觉肯定是AI的一个。

必不可少的一个component，但是不是AGI呢，这个就完全取决于你怎么定义AGI了，对吧，那我想我们在做，都是做视觉的，那这个它的重要性，这个就不用再debate了是吧，对对是的。

我们先建立了这个共识，然后这个，对，然后下一个，我觉得是现在大家，相信都非常感兴趣的一个问题，就是我们看多模态的任务，包括我们这个panel的topic。

叫Multimodal Perception and Generation，我们可以把任务，简单的分成Generation和Perception，那在现在这个时间点，就是大家觉得。

生成跟感知的关系是什么，然后多模态的生成跟感知，其实应该统一吗，然后这个，对我相信大家可能会有不同的一个看法，然后嘉惠要不你先来，来谈谈你的观点，因为你可能是这个，对接近的这个，对我是研究过程中。

是特别想把两个事情在一起做，然后我相信对我们在场所有的研究员来说，几乎就是说这个答案是肯定，就是我们希望能把这个事情，尽可能简化，尽可能去unify，我觉得就是，那有可能对工程来说。

不一定是一个中央的这种希望，就对研究员来说，是大家都喜闻乐见的事情，那这个问题我觉得，关键还是在于说我们有这个初衷，我们喜闻乐见这个感觉，就是说合在一起，但是能不能做到对吧，今天有没有足够多的证据。

说这条路应该是the way to go，这个我觉得今天就是说一个领域共识，可能还是还没有到那一步，就是说还没有到说，我们只往这个unify的方向上走，其他的跟unify不相关的，直接就丢弃。

这个我觉得今天来讲也还是不对的，所以如果今天来看，这个是不是应该unify，我觉得应该unify，但是就是还是evidence driven，就是说不能说我们一条路摸到黑，就是对整个研究领域来说。

不能一条路摸到黑，因为今天就是说以不unify的方法，也能带来很多任务上的这种突破，给大家带来很多帮助，好 谢谢嘉辉，听起来就是还是有很多的，这个研究的问题，我们待会也可以讨论到，然后 特特。

对 我其实跟嘉辉的观点是完全一致的，就是现在我们作为这个研究的本身，肯定是希望他们能够成为一个主题，但是现在主要是涉及到，我觉得工程上的问题，暂时来讲是没有看到这个点的，但有可能比如说是因为我们的。

怎么说呢，learning objective不对，或者是我们什么地方还没有做清楚，这个还不好说，但是退一步讲，我觉得所以这是一个非常值得研究的问题，但是如果退一步讲，比如说如果我们是否是因为现在的。

比如说现在的paradigm有局限性，或者说它没有成为一个，比如说真正所谓的general intelligence的，这样的一种能力，使得它两种方式不能融合，这个也不好说。

只能说这是一个非常重要的问题，我觉得作为研究本身需要解决它，好 谢谢特特，戴老师，我没有什么更新的观点，我完全同意两位的观点，所以我就不说了，谢老师呢，那我给个更definitive的answer吧。

我觉得一定会统一，并且我觉得应该需要统一，我个人想法是这样的，我觉得当然大家研究这个generate classifier，已经有很长时间了，然后并且从BS的角度来说，你其实这个PX given Y。

PY given X这两件事情，其实是可以互相转化，昨天Aditya也聊到对吧，就是your diffusion model is secretly，a zero-shock classifier。

这件事情我们已经能够看到一些signal，从我的角度来说，或者从representation learning的角度来说，我似乎觉得generate model。

是一个很不错的去学习representation的一个办法，因为简单来说就是说，如果你现在只model，PY given X，你的probability density。

其实只是在这个wide space上去compete的，不管这个Y是1000 labels，或者是fixed vocabulary的language，它还是一个比较离散的，比较有局限的一个空间。

但是如果你现在要model PX given Y，那你的density是要在整个的large的，viral domain上去compete的，这件事情必须带来的结果是。

我们要对这个image或者video，或者整个的viral content，会有更deeper的understanding，我们需要知道，比如说一只四条腿的猫，要比一只三条腿的猫。

更likely to happen，然后这件事情我觉得是一个，很non-trivial的问题，并且我觉得至少我个人而言，我觉得这是一个对understanding来说，一个非常critical的事情。

当然具体的实践上，我完全同意前面嘉宾说的，就是我觉得我们不知道该怎么做，但我觉得这也是非常exciting的部分，对 好 谢谢老师，我其实对谢老师有个问题，就是我觉得这个在理论上是很有意思的。

那你觉得这里面比如说，在probability上面的modeling，它到底应该是一个什么样的，latent space去model这样的probability。

比如说在generative model的角度来说，它很多时候是pixel space，那这个space它可能不是一个最好的，比如说latent space去学习这样，比如说四条腿猫。

三条腿的猫这个概念对吧，对 就回到样的观点，他说we should not work on gender models，because we don't care about this。

very detailed construction，we care about some real representation，in some form of latent space。

但我觉得这些都是open question，但是again maybe，这个final summary就是我觉得，看到SOL2之后我觉得，其实这条路线是有可能能走得通的。

然后具体的representation，到底应该怎么做这些，我觉得需要更多的research，沈老师在，对这个问题有什么看法吗，喂 沈老师，沈老师应该电脑又坏了，我们先待会沈老师再接下来，我补充一点。

就是我们刚才讲到生成和感知，在视觉这个domain，我觉得有可能我们需要再跳出来看，就是说有可能视觉和比如说文字，和其他的一些signal，就是说它们放在一起的时候，有可能当你做生成它就能带来感知。

然后如果纯视觉它也能带来一部分感知，但并不是，当我们讲比如说理解的时候，今天还是在把一些visual的concept，map到一个比如说texture的concept，比如说我们看到一条狗。

说这个是golden retriever，但它其实还是一个文字的表达，就是说以文字给它来原定，所以当我们在讲，就是说是不是生成和理解，要从一起来看的时候，就是有可能把这个视野。

再拓宽到其他的一些signal上，有可能这个答案会更肯定一些，就是更赞同那个赛敏老师的这个观点，是 对我们后面还，大家能听到我吗，可以，我刚才这个麦克风也出问题了，这个很奇怪，anyway。

对我补充一点，其实这里面有一个，从某些能力的角度来讲，就因为我们在谈感知，以前在视觉里面，几乎所有的perceptive的任务，都是用discretive model在做，对吧。

基本上从八九十年代一直到最近这三五年前，对吧，那么就所以从我们性能力的角度来讲，其实这就是generative model，versus discretive model。

那以前为什么generative model做不好，这里面当然最主要的原因，就是computing power和数据的问题，对吧，这个cosmics model就是一个generative model。

对吧，在之前小数据的时候，几乎是用处非常小，所以我的一个理解就是说，以前不是说大家没有意识到generative model的重要性，而是说受限于很多当时的限制。

他没有办法去把去训练非常大的generative model，没有办法把generative model的优势的发挥出来，所以不得已而为之去用discretive model。

用早期的subjective vector machine boosting，后面用CNN去训练一个classifier等等，对吧，那么今天就说从GPT的success到现在，最近这两三年时间大家看到。

这个generative model很显然是能够更好的利用大数据，对吧，他刚才Sandy老师也提到，这很明显，他可以把distribution学出来，那肯定直接比你去用discretive model。

直接学一个mapping，他能学到更多的信息，所以理论上来说，你如果能把这个generative model做好的话，那perception的很多问题可能就自然而然的，就做好了，就不需要用以前的那一套。

discretive model的这种做法，去解决这个perception的问题，我就说这一点，我能再补个很简单的，我非常非常同意沈老师的观点，而且我想在座很多学生其实是没有历史包袱的。

就是他是活在generative AI的时代，但回过头去其实可以关注一下，比如说甚至30年前40年前，比如说Alan Yu，然后包括我PCD老师的作文。

他们那时候有这个analysis by synthesis的学派，就是大家认为对图像的理解，对图像analysis，其实需要对图像的synthesis生成来做到的。

其实有很多非常有意思的discussion，并且Alan Yu最近也给了一些talk，在YouTube上都能看到，我觉得对我来说是很有启发的。

虽然怎么样bridge他们这些old school的concept，to something can be deployed in the current era。

I think it's another question，但是我觉得，again个人而言我觉得非常有启发，谢谢谢老师，然后我插入一个问题，本来没有准备，就刚刚嘉惠也提到了，可能大家可以考虑不同的模态。

然后我们现在想统一的，包括研究上的，大家除了生成和感知这两个任务，我们其实对于不同模态，到底哪些模态应该统一，我们应该一起考虑，然后哪些模态可能是分开考虑，大家现在好像没有一个比较成熟的共识。

比如说我们是图片和文字视频，还是说音频，甚至更多的包括robotics的信号，大家怎么看待在更统一的多模态的语境下，不同的应该用哪些模态，然后哪些模态可能带来更强的，这种scalable的能力。

然后它应该是有机融合的，这个对大家可以分享一点，就是自由选择，可以有什么可以，所有模态，比如说，比如说视频音频手势触觉听觉，包括所谓的情感，这样所有的东西应该都在一起，因为很难去想象。

至少可能是因为我们自身的限制，我们学习的过程永远都是所有的模态在一块的，而且很有意思的是，其实多模态这个点，主要在过去几十年里，很多时候是media study。

是学communication的人在用的一个词，其实不是在AI里面是最近才开始出来的，所以这就是汇到了一个学习和沟通的过程，比如说我用一个非常平和的语气跟你说话。

和我用一个非常aggressive的语气跟你说话，这个我所表达的信息是不一样的，然后这个信息其实对于文化而言也是不一样的，比如说在这里的文化，同样的手势可以在另一个文化里面代表一个完全不同的意义。

这就说明，我猜测它不是一个，它是一个怎么说呢，跟学习是一个互相交互的过程，你不能说我先学好了以后，我再去做动态，而是我觉得你要从头就开始，从所有的态一起开始学习。

因为我觉得这是一个非常comprehensive的过程，好谢谢特特，江辉有什么补充吗，我也补充一点好了，就是说我觉得动态本质，它就是在某一个时间点有不同的信号，它在同步发生。

那你为了就是说决定下一个时间点做什么事情，有什么事情会发生，那就是说你必须得融合当前这个时间点的所有的信号，所以这个在我看来就是说，更加确定一点就是说比这个是不是应该，生成和这个理解应该统一。

我会更加确信就是说，多个模态之间应该是要统一起来，然后统一起来会发生就是会能做更多的事情，好谢谢江辉，沈老师或者，对我说两句，就是说从research的角度来讲。

从我们在academia做research的角度来讲，那理论上来说，你可以把所有的模态，只要你有数据对吧，可以把所有模态的数据都用起来去训一个，这个transformer的模型，这里面其实很重要的一点。

就是说因为现在就是放在transformer，或者以前应该说是sequence to sequence，你这个框架里对吧，现在几乎所有的模态你经过一个tokenize之后。

把它把这个input做成一个sequence，那你就很自然的就bit到transformer这个框架里，所以现在这个框架来处理，不管什么样的模态都非常方便，那在以前是做不到的对不对。

在sequence learning出来之前，在transformer出来之前，你不像是两维的视频是三维的，然后这个audio是一维的等等，你都要去用非常复杂的feature extractor。

去做一个可以认为是pre-processing，所以非常复杂，OK现在非常方便，那理论从算法的角度来讲，你不管什么样的模态数据，你算法几乎不需要做改变，所以很方便，从research的角度来讲。

从application的角度来讲，那这个就取决于你到底想不要的什么样一个系统了，对吧，你如果做robotics的话，是不是需要把刚才特特说的所有的，能采集到数据都加进来，也许吧，但是anyway。

这个可能就从工程的角度来讲，可能会牵涉到一些什么data imbalance等等，这方面的一些问题了，好 谢谢盛老师，我稍微补一句吧，对我就说反正就是说这个肯定，就刚才大家说了就是多模态这个事情。

从科研的追求来说，或者从长远的目标来说，这肯定是一个非常有吸引力的事情，当然我自私自己，我们肯定也是在努力在追求，但是反正就实际做的时候，你真的要scale up，然后做一个性能非常强的一个。

多模态的大模型的系统，比如吃进去全互联网的数据这种级别的来说，它中间还是会有非常多的，现阶段的一些挑战的，来自于不同的方面，对就说首先我们观察到的我们的感觉是觉得，模态网上加的时候。

它的有时候训练的复杂度，带来的是几何乘数的在网上面在增长，如果你真的要训一个原生的多模态的话，当然就说这就是训练方法上的问题，然后以及所谓的原生多模态，如果真的要做到说是同一时刻。

然后几件事情同时发生的话，对你的数据的采集，然后什么的，就因为现在这一代的算算法，它其实对数据的利用效率并不是那么高的，就是成本这些会带来非常多的现实的挑战，所以我个人的观点，我会觉得原生多模态。

就是说从头开始训，所有模态加进来同一时刻什么的，这种训，我觉得会是一个非常美好的长远的追求，当然在现在的时刻，我觉得有很多的现实的问题，需要去解决，然后以及有些事其实会。

促使我们去反思这一代的AI的训练算法等等，中间存在一些本质的问题，比如说有灾难性遗忘等等，带来各种各样的问题，说完，好 谢谢戴老师，我可以说两句吗，可以可以，对 我完全同意，所有模式都重要。

但它们并不相等，我只想说一件事，就是也是回到之前的讲座，我觉得语言这个东西，可能要考虑单独拿出来看一看，就是第一是它太强了，会导致会影响其他模式的学习，第二是这个东西真的是人类特有的东西。

然后至少对我个人而言，我还是比较希望能够看到某种意义上的，动物智能或者猫猫智能，在AGI之前的，当然我觉得这是一个长期的研究，我可以强调一点，当我说到所有模式的时候，可能我还是比较赞同赛宁的观点。

就是所有的模式应该在一块，但是我们不想要看到一个模式，去主导其他的所有的模式，尤其是当这个语言，比如说赛宁在讲座里面也提到了，就是语言上你非常容易有种欺骗的感觉，因为它是一个所谓的人类特有的。

至少我们认为是人类特有的东西，当它有的时候，你可能很快地把它跟智能连到一块去，那这时候会发现我们在评估一个系统，它到底有没有智能的时候，可能会有对语言的偏见，那这也是一个问题。

但你说我们比如说做一套系统，一个人类计算机互动，它也必要有语言，它一定要有语言，不然我们没有办法跟它沟通，可以没有语言吗，我觉得是，我觉得在一个理想的世界里面，是可以没有语言的。

因为比如说假设我们人不存在，那我们就假设另一个智能体吧，把它们丢到地球上，那它不一定都一定要有语言，它可能只要有一些简单的这种沟通方式，甚至假设如果它们都有什么，我们从科幻的角度来说。

它们可以用无线电信号去互相沟通，那它把它的hidden state发给你就好了，其实没有必要用语言去跟你沟通的，对吧，语言是一个人特有的东西，它是一个非常compact的东西，是因为我在这说话的速度。

我不可能就跟你用这个，无线电信号这个速度去跟你沟通，所以这个我觉得是一个现实逼出来的一个东西，而不是它说是不是一定要一个，智能必要有的东西，谢特非常棒的这个观点，我们move on到下一个问题，然后。

我们刚才讨论到了有不同模态的任务，然后包括也有生成感知理解的这些任务，但是我们目前好像还没看到，一个类似CHI-GP这样，大家所有人都能感知到的这种突破，当然大家可能有不同的看法。

我想问一下在座的各位老师，大家认为对于多模态的，不管视觉语言或者是其他模态的，生成的感知的理解的这些任务，什么时候或者达到什么程度，算是实现了对应的CHI-GP的moment。

然后或者说我们现在已经有了吗，还是说还需要更长的时间，来让大家都能感出到的这种，多模态的真正的类似CHI-GP的这种moment，我现在回答一下，对其实我今天的PPT里面有提到，我的个人观点是那样的。

就从生产力发展的角度上来说，就是文本这个模态的CHI-GP的moment，我认为它具有的特点是那样的，首先第一点，它在一些重要的任务上，它的性能足够好，然后第二点的话，它是能够低编辑成本的。

泛化到开放的任务上，就说你一个CHI-GP挂在网上了，你可以服务上亿人的，千奇百怪的各种各样的请求，而你服务每一个请求，它的后面只有计算的成本，并不需要研究员，很贵的研究员对吧，比如说在线的很贵的。

佳辉对吧，这样的研究员为你每一个模型去进行一个，每一个绘画进行一个特调，对我觉得就是作为一个产品，它到了这样的一个时刻，我觉得它才具有这种爆发，或者说就是让大大大大大。

就是切实地改变现实的这种生产方式的能力，对所以同理我觉得多模太CHI-GP也是一样的，就是说反正在一些重要任务上，具有非常强的这种性能，同时它应该要具有低成本，低编辑成本的泛化到开放的。

这样一些请求和任务的能力，这是我的观点，谢谢大老师，佳辉线上非常贵的研究员，有没有什么补充，对对对这个问题来本身来讲，我觉得有一点我是不太同意的，就是说听上去好像这个语言的发展非常快。

然后视觉多模太就是说发展比较靠后，我觉得其实不是这样，就是说你也可以讲就是当年，比如说CNN出来的时候，比如说Minist大家能认识的时候，我觉得这个也是CHI-GP的时刻，因为大家这个寄邮件什么。

你不需要人去栽抄这个邮编对吧，这个其实也是服务上千万用户，然后包括你可以说比如说ResNet出来的时候，最早也是在一个视觉上，所以我觉得CHI-GP的时刻可能更多的是。

就是说每个领域的这个Milestone，大家都有多少，然后是不是在持续的推进这个Milestone，就是说对于这个问题本身，我觉得没有特别赞同这个问题本身，好的谢谢江辉，我稍微说一句吧。

就是我觉得这里面还有个baseline的问题，就是CHI-GPT之所以好用是因为，它能写出来这些东西我是真写不了，它能讲的这些英文我是真讲不了，但是一个多模太的模型。

很多时候这些回答尤其它比较Vision Centric，或者说像大老师说出来，比如Vision LRM里面我们能support，比如说Process， Detection， Segmentation。

一个三岁四岁的小孩就能做到，所以我们对它的这个expectation会更高，我觉得这件事情可能是一个，怎么讲呢，我觉得即使是一个挑战也是一个机会吧，就是我们一定会对这些多模太的。

尤其是Vision Centric的task，会有更高的expectation，会对它的错误更不能容忍，which means这些系统，得要有更好的reliability和robustness。

我觉得它才能够真正，能够真正被我们日常生活利用起来吧，好谢谢老师，没有补充吗，没什么补充，那我们来到下一个问题，就是对于现在的很多视觉语言任务，包括我们最近两年看到的突破。

可以看到是有两条技术线在推进的，一条是这个，比如GPT为代表的autographic的model，然后我们通过预测下一个token，去在语言上面大规模去训练。

然后另一种像diffusion model这条线，包括最近的SORA，然后相当于dominant的两条线的发展，然后那这个问题就是说这两条路线，就是分别的优势是什么，然后哪一条路路径潜力更大。

我相信这也是大家很多人，关注的一个问题，然后看在座的几位，对有什么各自的看法，教辉好像你也是最接近的，对于这个问题来说，对我觉得就是说首先两种方法，本质上都是把一个比较复杂的问题，给拆开来，拆成多部。

然后用大量的计算，在每一步上用大量的计算，然后再多部合起来，就是autographic的model，就可能比如我们以前，一个很有意思的事情是，当我们做这个图像生成的时候，能发现拆成1000步左右。

是比较好的一个chill off，然后我们去看diffusion的model，发现它也是1000步这样训练的时候，所以本质上是，我觉得很像的做法，但是有一些区别是这样子，就是当我们比如说应该。

放更多的精力在哪一个方法的时候，首先肯定我还是只有，就是说要open minded一些，不能说一条路走到死，然后其次就是当我们在，真实在评估这两个方法的时候，我觉得如果说diffusion这样。

这一类的方法，想要成为比如说一个，一个universal的一个solution的时候，可能更多的是要去看，比如说它对文字的这个处理，能不能达到像这个，autoregressive model的这种效果。

那我们反过来再看，就是说对autoregressive的model，我们今天有人做比如说图像生成，有人做这个视频生成，相对来说我觉得，离这个diffusion的结果更近一些，所以比如说两个方法要。

就是说方法压住的时候，我觉得这个autoregressive，在我心里会分量会更重一些，然后diffusion就是说不容小觑，不要就是不要这个封死，对，好 谢谢江辉，沈老师线上有什么补充吗，我补充一点。

就是说因为你刚才提到，diffusion做生成，那我提供一个，就是我从另外一个，从这个图像的representation，learning的角度来比较这几个方法，对吧，其实到现在是没有一个定论的。

就是说你如果是把它做成AR模型，那这个戴老师最近的interview，你今年那篇CPU的paper，已经做了很多downstream的这种task，就是说放在多模态里面训练出来的。

这样一个imaging code，它的能力非常强，对吧，可以去做很多detection segmentation等等，都能做得不错，那也就是说那些AR这样的一个训练方式，可以训练到一个非常好的。

一个imaging encoder，imaging representation，从另外一个角度来讲，因为刚才在沈老师也提到clip是吧，就你clip的话，你其实它就是在训一个分类器，对吧。

然后它用的是，它用的信息其实都是一样的，都是配好的image加上text description，对吧，跟你训多模态是一样的，然后defi model其实也是一样的，对不对。

defi model你训的时候也是用的，用到的监督信号都是配好的image加text，那么现在我们最近做了一个简单的实验，就是把defi model。

就是stable AI预训好的defi model的unit拿出来，就做一个initialization，然后去做很多下游的这种segmentation等等这些任务，这个效果也非常好，那么我想说的是。

你看这几种不同的训练的方式，如果我从representation learning，imaging representation learning的角度来看的话，其实效果都不错。

但是到目前为止也没有一个，我没有看到一个公平的一个对比，说到底哪一种方式，训出来的这样一个encoder，这样一个representation，更powerful，其实我没有看到，anyway。

回到你原来的问题，就到底是哪一种方式更好，我觉得这取决于你到底想干什么，对吧，如果说从考虑representation learning的角度来讲的话，我现在没有一个答案，我没有看到一个答案。

到底哪一种方式更好，也许最后结论是差不多的，是吧，在相同的训练数据，相同的训练信号的条件下面，那我就说这一点，好，谢谢程老师，然后我们下一个，还有补充吗，还是我们，都行，有时间，有时间，还有半小时，对。

我可能简单补充一下，我觉得可能这个问题，最后it doesn't matter，就是我就还是相信，the beta lesson，可能最后真正dominate的东西，还是说你的computer在那之后。

可能大家都行，但这里面会有一个inductive bias的问题，就是我个人觉得autoregressive model，对language来说还是比较自然的，或者discretized tokens。

diffusion model对viral data也是比较自然，比如它会有course defined，会有这种multi-scale的概念在里面，或者说但这两件事情反过来。

比如说我非要去assume一个raster scan order，然后去从左到右，从上到下的去扫我的一张图，做autoregressive，对我来说似乎不是一个很自然的inductive bias。

但again我觉得可能compute上去，不一定会有太大区别，但是也许在某些场景下面，这些inductive bias可能会变得比较关键，所以我会倾向于从这方面考虑考虑，谢谢，看到我。

对我其实我很赞同这样的观点，因为之前的话，其实大家像我之前做了一些工作，比如说在看这个CNN和VLT之间的区别，那后来其实就zoom out的一点点，发现到最后可能还是。

就是compute或者说是data，就是在实际的意义上来看更重要，那么在这些model architecture，到最后就像是提供了一些下游task，inductive bias。

然后还有就是在实际意义上的选择，比如说VLT它确实或者说是transformers，它确实非常容易去model各种各样的data，而不是说就是它，而且它是scalable的architecture。

那么这两点加起来，可能是更多人选择它的原因，我觉得可能整个领域在过去有一些变化，就是大家zoom out以后，可能看到的更多的是，OK我在同样的数据，同样的compute下面，达到的效果是不是差不多的。

现在看起来好像是差不多的，但是还是没有完全的定论，可能还是the beta lesson，好我们下一个问题可能是一个具体一点的，就是我们讨论到刚刚视觉语言，主要是视觉或者视频更多模特的时候。

一个很难避开的就是encoder的问题，或者叫tokenizer也好encoder也好，我们怎么把模特的数据，给它编码成某一种表征，送到后面的模型，然后这里包括我刚刚报告里面，其实也分享了一点观点。

就是现在好像有一个不可能三角，就我们对图像或者视频的tokenizer，很难兼顾compact lossless和离散，就是紧凑无损和离散的三个方面，可能现在最多有两个方面。

然后而这个问题可能对于我们未来，想要做更统一的动态，可能也是一个核心的研究的问题，然后也想看在座的，包括线上的几位老师有什么看法，教辉好像这个也跟你最相关，对于不可能三角就是说，确实我觉得是不可能。

因为文章说解决了这个不可能三角，比如说它既是离散的，然后它又能表达得很好，然后它跑一个比如说像这种MS COCO，captioning task，其实当你看这个captioning task的时候。

大量的这个，当你看这个eval的时候，它其实还是这个文字的style，决定了你这个eval的高低，并不是说因为它里面可能就一只狗，它说一只狗在跑，那我现在换一个问题，我说这张图片。

比如说这个从左到右第五个pixel，从上到下第十个pixel，它RGB里面的这个R的值是多少，那基本上是没法做的，所以我觉得这个不可能三角是不可能，那更有可能的反而是说。

针对某一个点比如说针对lossless，我们在什么样的task上，希望它是lossless，然后或者说针对于这个discretization，我们是不是真的需要discretization。

discretization有什么好处，就是我觉得可能从每一个点去，或者找一个点去突破，反而能打破这个不可能三角，比如说我举个例子，能看到一些领域里面的一些工作。

比如说这个就不通过这个discretize，然后来做一些生成，然后同时它又能把理解做得，因为它这个带宽还是比较宽嘛，就是那个成果的，或者说这个lossless这一块，有可能就是我就放弃这个。

我要seek这个pixel value的这个task，我就是比如说做一些robotics的task，有可能能出来一个很好的tokenizer，它又是discrete。

从这个pixel的意义上它也是lossy的，但是它能lossless完成这个robotics的一些task，对，那我谈我的观点吧，好，谢谢嘉慧，我完全同意嘉慧的观点，我觉得从就是从Homeri的角度。

你如果要，你不知道在下游task是什么的情况下，这个你是不可能去，很难去做压缩，几乎不可能做压缩，因为我完全可以去构造一个task，使得你被压掉的地方，是我要问的，就是我要问的问题。

那这个你是做不出来的，我觉得到最后而言，就像嘉慧说的是找到一个balance，在什么task上去需要一个什么样子的指标，我觉得可能lossless是一个，如果是我的话，是我一个先会放弃的地方。

是因为没有任何的生物体是真的lossless，因为如果lossless的话，说明我们要存exactly这个光子，打到我们的视网膜上的每一个次级，that's impossible，对吧。

而且所以这些对执行大多数的task是没有意义的，我就说一句，我觉得tokenizer是一个fancy name，但我觉得我们讨论这些东西。

其实是originally what soft supply selling was signed up for，就是一开始大家要做SSL，目的就是为了去学习一个更compact。

更maybe better alignment，aligned with the semantics，不用说lossless，但是比较adaptive或者说比较flexible。

能够去solve一些task的一些东西，所以我觉得其实，至少在我看来，我觉得这一部分之前的soft supply selling approach，肯定是没有达到我们的预期的，但我觉得之后。

这也是一个很好的发展的方向，好谢谢老师，大老师或者沈老师有补充吗，我没有补充，好的好的，那我们对跳到下一个问题，就是，对这个问题可能大家自己可以选择回答，就是我们现在大家经常会讨论。

这个语言里面的skilling law，然后那对于视觉视频，就是包括图像视频或者多么hide里面，它的我们现在已经看到了，明显的skilling law，然后它如果没有的话，它可能是什么样的。

对大家可以，要不谢老师，其实我自己也不知道，我只能说可能在VN里面，我个人觉得我们还没有看到skilling law，至少没有看到这种language的GPT moment，这种counterpart。

然后我觉得，这里面还有一个重要的问题是，这个benchmark到底是什么的问题，然后我觉得对VN来说，想要看到skilling law，或者我们叫它skilling observation吧。

也需要依托于一个比较成熟，比较reliable的一个benchmark，或者说至少是一个evaluation的protocol，我觉得这一部分，我们也得要想一想该怎么办，线上的两位老师有什么补充吗。

我觉得如果拆开来看的话，对于这个生成来说，skilling law还是相对来说比较好做，对于理解来说，因为这个理解的任务，很多时候它是跟这个文字连接在一起。

导致就是说当你看这个skilling law的时候，是在看这个文字的skilling law，还是在看这个视觉的skilling law，对吧，就是这个会没有那么清楚，但有一种做法就是说。

我通过这个刚才讲到的对吧，就是说通过生成来做理解，那我觉得这里面可能会出来一些skilling law，并且这个skilling law，可能跟下游的一些，你说care的任务有一些相关联。

这个会是很有意思的研究课题，好 谢谢江辉，我们再补充一点，好，再补充一点，就是说skilling law，还见到过一个我觉得是比较failure case，就是说。

当一些vision的研究员看到这个文字的模型越来越大，对吧，从几百兆到一个币，然后到100币，到一个圈，然后就是有点心想一样对吧，然后也去skill这个vision code，上来比如说一个币。

然后5个币，20个币，100个币对吧，这个我觉得盲目skill，不是一个好的东西，这个还是啥，好 谢谢江辉，我其实想就再补充，简单补充一下，就是指盲目skill，因为这个让我想到了。

就是因为我自己是aviation爱好者，所以如果我看到比如说过去这个航空发展的历史的话，有一个阶段是从比如说更快，就是从莱特兄弟以后对吧，这个有现代的航空学，然后再到一个阶段，就是大家永远都要造大飞机。

为了造更大的飞机而造更大的飞机，造更大的飞机，所以比如说这个747应该是没记错的话，是60年代的产品产物，70年代可能已经开始开始使用了，但是你看现在的话，比如说747也好，或者或者380也好。

其实已经不太产了，对吧，反而与此这个取代的是可能没有那么大的飞机，可能是350或者787这样的更加优化的产品，所以我觉得就像盲目scale，到最后它要有一个就是我为什么要去scale。

有可能比如说我用更好的空气用力学，可以做出更省油 更经济，更先进的飞机，那么我为什么一定要用这个，要造得更大 更重 更耗油呢，这是一个很有意思的问题，非常形象的一个对比，我很想说一句就是有时候。

也不一定真的是盲目scale，比如像你们做这个EVA CLIP的这些model，有时候你要问我说这个model到，比如说180个边缘或者怎么样，到底performance应该怎么，我其实不知道。

所以而且我自己也做不了，所以我看到这些结果之后，虽然可能说没有想我们想象那种skin-low，对我来说还是一个很有价值的一个data point，所以或者说只有有一些人做这些事情。

我们才能够不去盲目scale，就里面还是有一些research inside，好 然后我们下一个问题，就刚刚可能也提到的就是，数据可能是现在大家都知道最关键的，然后最重要的一个部分，然后特别是。

因为语言数据可能相对来说形式简单一点，那当我们想要讨论到更多model的时候，它就是我们现有的数据的数据量，以及数据形式数据质量，是不是满足我们未来期待的动态的能力的，就包括刚刚畅想的能力的要求。

然后未来的形式可能是什么样的，然后它可能是来自哪里的，就这里我们可以看到之前包括图文队这种数据，在clip各种的成功，那未来我们应该期待什么样更新的形式，这里也想看各位老师有什么自己的看法。

我如果self serving一点的话，我会继续说就是real world data，因为就像我的talk里面提过的有几点，一个是internet data永远总会有一天会用完，这是一个。

第二个是internet data和real world data，还是有domain gap，这是第二点，所以我觉得all the evidence leads to。

就是大家要更关心的去在real world里面，如何去采集数据，或者是利用这样的环境去学习，我说一下我的观点，首先第一点呢我完全赞同特特说的，就是我会觉得就是说，多模态模型未来它的最重要的应用场景。

还是说是能够主动式的去跟现实世界打交道，就是说机器人啊什么这样，就跟现实世界进行主动的环境的交互，就是说它不止观察，被动的等着人来喂给他网上数据，而是说它主动的在环境中间去看，去交互去听。

我觉得这个会是创造最大生产力的地方，然后第二点呢就是我会觉得，其实这个会引申出一个问题，就是说其实就是我觉得，现在的学习的算法，就是说它的那个效率什么，可能还需要进一步的提升，尤其你跟现实世界去打交道。

这些数据其实都会更加贵，然后更加稀缺那样的，然后就是说以及你看，像小猫小狗或者人类的孩子，其实它并不需要那么多的语言的监督，它就能够学得非常好，小猫小狗根本就没有语言的监督。

但别人照样会很好的跟环境去打交道，对就就是说我觉得，其实这个会引申出来一个问题，就是说反正我觉得现在这一代学习算法，其实还有需要非常多的改进的地方，对，对我可能稍微有点悲观是，我觉得这个数据可能。

如果尤其是scalable数据只能被discover，但没办法真正去collect，就像我们做project，我当然完全同意这个real world的distribution，是非常非常重要的。

但我觉得有时候一些数据可能要依托于，下一代的某种硬件平台等等，我们才能够去discover这些数据，我现在看起来一个比较promising的数据，或者说在中间的状态。

可能还是internet scale的video吧，但它主要的问题像大老师也说，这个不是一个interactive environment，但也许我们可以通过像用生产模式。

用现在各种3Drepresentation learning的技术，可以让它somehow能够，bridge the gap between real and synthetic environments。

然后再去develop新的算法吧，对这是我的一个hope，谢谢老师，先让江辉或者沈老师有补充吗，我说一点就是除了NLP以外，其实现在NLP self-super learning非常成功。

但是现在在别的模特里面，3D点云也好，图像也好，视频也好，我们现在根本就没有看到，一个能够工作的非常好的一个，self-super learning的一个方法对吧，就是说现在你动物态还是在。

就用配好的text和图像在做监督，如果还是以这种方式的话，没有在self-super learning这块没有突破的话，我们就去收集这个数据，你连应该收集什么样的数据你都不知道，对吧。

你只是去collect那些，在recorded video也好，image也好，你标注成本会非常高，如果还是以目前这种学习的方式的话，所以anyway，我想说的是这里面可能还是有非常多的，工作需要去做。

是的，谢谢沈老师，江辉呢，我也是一样的观点，就是说可能重复一下，就是说一个是，我觉得数据量不是问题，可能关键在于怎么学，然后学习的上法是什么，比如说能不能有self-supervised的这些事情。

本质上这个世界时间只要再往前挪动的话，它动物态的数据就会往前去增加，但是能不能把这些数据用起来，然后能不能把它用到自己感兴趣的一些任务上，这个可能是需要更多研究的地方，好的，谢谢，当然还有一点就是说。

就是因为问题里面还有说数据有可能从哪里来，对吧，今天立体还在赶飞机没来，就是说我同事就是，图像生成我觉得也是一个很好的数据，是有可能可以用起来，然后有可能跟新的学习的方法，如果能有一些共同的突破的话。

会有一些很有意思的东西出来，好，谢谢江辉，然后我们留几个问题给现场的观众，然后看大家有想提问的可以举手，然后我们工作人员可以递麦过去，那里，我想问一下那个赛琳老师讲的第一篇工作里面。

就是我记得当时是把clip换成了，加上了一个dinov2，然后当时第一部分在找那个反例的时候，说的是找了两张图片，然后他们的clip的距离非常近，但是dinov2距离非常远。

然后这样来说明说这个clip可能，visual grounding不够好，那比如说我把那个clip直接换成dinov2，然后找一个dino距离很近，但是clip距离很远的。

那这样的话会不会它也可以找到一些反例，对，就要澄清一点是我们没有在做某种，adversarial training或者whatever，其实这些embedding space上的discussion。

都只是帮助human annotator去帮忙label，所以human annotator还是在中间一环，需要去控制这个流程的，所以我后来也收了这个result，其实我们没有试你说的这件事情。

但是至少比如说这件事可以去把dinov2，换成其他的soft supply slendering model，这个conclusion也是成立的，对，那有没有试过就是再加入一些其他的feature。

就比如说把soft supply slendering model，也多加几个进去，这样会变得更好吗，欢迎关注我们之后可能会release的，一个paper，我们会讨论一些相关的问题。

short answer是可能没有那么直接，不是越多越好，对，所以就是这个soft supply slendering model，跟clip之间还是有区别的，是的，谢谢，好，我们可以看下一个问题。

请问一下这种视觉模型需要增强它的可解释性吗，就是可解释性对这个视觉重要吗，问哪一位老师，都可以，先从你应该回答一下，对吧，在座的回答吧，您回答一下，沈老师，沈老师有什么，我觉得不重要，能work就行。

不是一定要有可解释性，有当然好，没有也不妨碍我们去用，是的，但是如果是用于医疗的话，他可能是需要有很精确，他是怎么得出这个结论来的，如果把这种影像和文本用在医疗上的话，他是不是需要可解释性。

然后怎么样去发展这样的一个模型，我觉得这个也有点绝对，其实很多很多很多以前不是Google写了一篇paper，然后忘了是做哪是skin cancer吗，还是我忘了，不是skin cancer。

是retina，眼底眼底retina scan image，就是眼睛的，这个叫什么，anyway，就发了一篇的science，对吧。

去用retina scan image去去去去detect diabetes，他的训练级的数据级，他是每张图片找了七个端，七个specialist，七个医生去标的这个做的这个标注，对吧。

完了之后就是modality voting，对吧，那这个七个里面如果有四个同意，那就那就用用这个观点，对吧，我想说什么呢，你看这个医生去标，七个医生去标这张图片，大部分情况下都标出不同的结果来。

这个这个这个这个这个训练，训练好的这个医生，他他能解释吗，他也不能解释啊，那那你到医院去不就是在这样的医生在帮你看病吗，你不也接受这样的结果吗，所以这个还是case by case吧，我觉得对不对。

对对，是的，我我我那个，对，教会，我斗胆那个提一个跟陈老师不一样的观点，我觉得很重要，就是一方面的是解释性能带来一些新的insights，然后其实deep learning。

他能到今天我觉得还是非常magical，就是突然一个时间点，所以给你这个技术，然后这个技术能到今天的这个结果，然后在可解释性上的研究。

有可能能带来一些deep learning对deep learning的理解，然后其他下一下就是说下一个突破，尤其是就是说如果你今天是研究这个可解释性，我是非常鼓励你继续研究下去。

不要说这个改topic，因为有可能就是说毕竟研究下去有可能是下一个，比如说scan，跟low impact level的研究，相信自己就是扣除可解性，对我我简单就是echo一下，教会刚才说的。

我理解的interpretability有好几种，一种是工程上和实用上的interpretability，比如说OK，我这个系统在医疗上用了以后，我怎么知道它是通过什么样子的逻辑，来做了这个预测。

这是一个实用性上的可解释性，但是另一个可解释性就像，嘉惠说到的可解释性是OK，我能够指导我们以后的研究的方向，能够指导我们，给我们提供deep learning这个过程中的insights。

我觉得在这样的可解释性的基础上，它会OK，那我们假设吧，以后知道往哪去scale，用什么样子的数据，那是不是我们就能够更好的把这个领域往前推下去，更快地去把model带到下一个level。

而不是说我们所有的东西都要试一遍，所以我觉得在这个角度上的话，我觉得是必要的，但是如果是在实用性的角度上来说的话，就像大家已经讨论过的，这个在有些时候是需要的，有些时候是不需要的，还有我们。

对那边有个男生一直举着，说吧，我想请问一下各位老师，那个随着我们在这种视觉语言模型当中，加入了更多的视觉的训练数据之后，在纯语言的性能上面，比如说reasoning相关的能力上面。

我们有看到一些比较大的提升吗，以及在可见未来当中，大家怎么看这件事情，谢谢，这是一个对很好的问题，我只我看到GPT-4V的tech report的时候，看起来没有特别大的提升。

所以我们就要去问一下嘉辉了，嘉辉，你跟第五周，不方便透露，看来，我觉得这个问题本质是这样的，就是说，当而且跟之前问的问题有点像，就是说我们几个模特是不是要和在一起做，那这里面本质上问题是在于。

就是说互相之间有没有这种positive transfer，就是你做了这个问题，能不能transfer到另外一个问题，然后，然后甚至有没有可能就说有negative transfer。

negative transfer我觉得还是一个可解的问题，因为positive transfer这个就说，只能说God bless deep learning。

希望是能有一些这个positive transfer，那有些情况下，我觉得还是很容易能看到一些positive transfer，尤其是比如说语言对这个动脉台，就是语言上比如说很简单的例子。

就是说你的这个语言的输出更加的标准化，会导致在比如说这个把你图文问答的时候也会更加标准化，就是更加符合你的预期，更加让你读起来通顺，那就是说图片这个事情能不能对。

因为你的问题是图片能不能对这个语言reasoning带来一定的提高吗，我觉得这个是有可能的，然后这个是研究中的，好的，好的，还得我们可能都得还得研究一下，我们补充一句。

就我觉得还有一个evaluation benchmark的问题，我一定能找到一些task没有vision，你的language model completely fail。

我也可以找到更多的task没有vision，压根就是这个language performance没有任何的影响，就是所以还是取决于我们到底应该怎么去define这个problem。

但我相信比如说如果回到刚才特特说的，如果我们care real world这些task的时候，我相信有很多很多的task是没有vision是不行的，好，戴老师或者沈老师有补充吗。

对就回到你刚才那个问题的话，就是现在的情况是至少我们这边观察到的情况是，就是说是你的问题刚才是说是那个对于语言任务来讲对吧，然后图文的这个多模态的训练是否对存语言任务有帮助。

对然后现在我们观察现象是没有帮助，甚至可能会有一些坏处，但就是说确确实实这件事情，如刚才各位所说的，反正是是有各种各样的空间，以及现实中间很多任务，就是说它其实是就是说反正越往现实世界走。

那就视觉这些东西的重要性会越来越多，对所以所以这个是我们的，呃到benchmark的问题，还是说这一代算法有局限性，很好的追问，不是你不能criticize说benchmark有问题。

那存语言任务的benchmark别人就定义的挺挺挺好的，对我觉得还还是就是说是现在的现在的算法的问题，或也也不一定你说这一代算法的问题，我觉得可能是也许是一些工工程设计上面的问题什么的，对。

好然后那我们还最后还有一分钟，要不最后一个问题，呃，那位那位那位同学举得最高，你好我有我实际上有两个问题，一个一个一个，我问一个问题是这样的，就是呃我有一个疑问是说。

因为现在token的话实际上就是一张图在token，就是encoder的时候会token成比如说呃一个就是，比如说1024个那个什么呢，就是token吧就是图片token以后。

那么的话如果每一张图如果它的分辨率都是1024，那么的话它token化以后它是这些token的一个组合，那么这个组合它代表的意义是什么呢，它是一个呃它是一个排列组合吗，还是到底它有更深层的含义，嗯。

对就是关于对这些token的理解，对就是它虽然都是10比如说都是1024分辨率，可是它最后的我们就是比如这个token的那个密码本，实际上都是1024的，那么它实际上都是这些的组合。

我不知道它的那个内部的含义到底是什么，就是说就像一个文章一样，我们的话token完了以后是一些呃，我不知道它是最后我是把它类比成一个，就是说像是一种token写的一项文章一样，我不知道是不是这样能理解。

这个问题我可以稍微说一下，就是说我几年前看过这个codebook里面的这个code，就是做法也很简单，就是说你把整个这个1024的token都要转成同一个code，然后你把它decode一下。

你看看你出来的是什么东西，然后我们观察到基本上还是一些很基础的颜色，然后有一些这种边边角角啊一些pattern，所以我可能更倾向于说在这些排列组合的情况下，加上你的这个decoder。

它才能把整个这个视觉的信息给decode出来，那回过台就是跟刚才那个问题很像，就是说要不要科技解析性，我的我的我的回答是需要的，就是我们可能需要有一些科技解析性的东西，来看一下这里面到底在干什么。

是不是人能理解，然后是不是能理解之后能把这个事情做得更好，谢谢，好那ok，ok，ok，那我们今天正好时间到了，然后今天也很感谢这个大家在这边一起参与我们这个panel，然后也感谢各位线上线下的这个嘉宾。

让我们有更多问题可以这个下面再讨论，然后也感谢大家参与今天的这个东派论坛，然后下午应该支援大会还有一些其他论坛也欢迎大家关注，谢谢大家。

