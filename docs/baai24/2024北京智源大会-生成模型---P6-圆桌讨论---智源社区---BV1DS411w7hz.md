# 2024北京智源大会-生成模型 - P6：圆桌讨论 - 智源社区 - BV1DS411w7hz

好我们今天这个报告courage还是比较全，这个文本图片和视频模态都覆盖了，那难得也这么多这个同行聚在一起啊，呵呵探讨一些本质一点的问题，对我们之前也准备了几个问题，首先呃就一上来可能就是一些灵魂发问。

首先是这个泛式的问题，我们现在至少有两种范式，这个扩散和这个自回归，还有很多可能登上舞台的范式，比如说MANA，然后那那后面这个范式会怎么样转变，嗯想想请这个各位老师谈一下看法。

或者各位嘉宾也可以结合自己的领域，谈一下这个看法，对我想要不从咱们按报告就按报告顺序，就是说从罗老师开始这么难的问题，对嗯，嗯这个方式方面，就是说我只能说通过我的经验吧，就是这个肯定不是结论啊。

嗯就是我我自己做多模态大模型做的特别多啊，里面嗯就是就理解模型就是输入是多模态的，但是输入是文字的，嗯在这个做的过程中间，我们同时也其实特别想把呃，视觉的生产结合进去，反正发现特别困难啊。

所以这种我我觉得啊，至少如果从一个有一个统一的，大一统的多模态大模型的话，嗯我们想生产的部分，如果嗯如果是比如说都是一个transformer，或者怎么样的话，反正至少现阶段我感觉还是有点困难。

嗯明白啊，这是我的经验啊，不是结论嗯，明白对对，我们多收集一些输入，对啊，姜老师，您的看法是什么样呃，我我的看法是呃，在算力相对没有那么多，或者是呃model size没有那么大的情况下。

我比较倾向于DE呃，就是扩散模型可能会能快速达到一个，还不错的效果，那随着算力的增加和model赛增加，我是比较相信那个AR的这个这条路线，能够达到一个更高天花板。

当然前提是他的code book做的足够好，对包括他的那个嗯这个TOGANIZATION的话，可能是一个视频和图像上联合建模的，对包括他的code book的呃，对这是我的看法，这样的差异在什么。

来自于哪呢，为什么就是这个scale会成为我我我我的，我的意思是说，就是呃因为那个呃就是AR这条路线的话，就是它呃就未来的话，它的scanning的空间会比diffusion会更高一些，对啊。

随着就是它的，就随着算力的增加和model赛增加的话，就是它的呃，就是后面会追赶上diffusion在某个节点，并且表现出更好的效果，嗯好的明白，对我感觉这也是很很有意思的观点，对孟孟老师有什么看法。

谢谢我其实也不是相关领域的专家，就抛砖引玉一下，就是如果从文本这个角度来说，其实这一直是NOP，还有ml这个领域的一个一个大家关心的，就是怎么做生成文本对吧，这个事情其实做了这么多年了。

在深度学习的时候，就最开始应该是RNASTM这种，到现在呢，可能就是以transformer为这个主流了啊，基本上是主流了，那其实他的学习的这个机制，本质上大家还是以自自回归为多啊。

auto regressive为多，然后架构上也也在transform为主流，但其实这两方面其实也有新的挑战呀对吧，其实你从学习的目标上，现在其实我关注到有这个BFN啊，这样他也可以做很多。

就是basin flow network对吧，他也可以做这种离散的数据的这个建模，包括呃之前有什么diffusion bird啊，还有就是类似的就是应该是有很多，就是建模上大家其实也在尝试不同的手段嗯。

模型架构上更不用说了，就是像MANA，还有这个啊，前两天看到这个新闻是嗯，RN又重出江湖了什么的，就嗯在架构上我想大家肯定也会啊，接着探讨，因为transformer它这个复杂度。

还有它就是为了解决它高复杂度，大家用的这些trick虽然说发展的很多了，但是如果说我们真的能把它这个推翻的话，其实你你你这些trick其实都没有必要的，就是我如果能真的搞一个这个线性的这种，这种机。

这种模型的话，对，所以我觉得其实嗯，这当然你要证明这件事情也很难，因为嗯因为sky nore这个问题，你必须搞搞到那么大规模之后，别人看到东西可能才会白银，但是我觉得。

这正是因为有这些不确定性或者质疑吧，就是呃大家才有这个去去做这些东西的机会，不然大家都看到了之后，那又那又就是可能会比较卷对吧，诶好谢谢，好封建有什么看吧好呃，这个我我我我我我我也是一个自己的想法。

不一定对，就是呃我我我最早思考这个问题的时候，是从纯纯数学的那些角度或者怎么样，然后发现其实没有什么结果啊，就是呃虽然刚才那个呃谷老师质疑了，这个diffusion是最大自然的这个。

但但我之前一直是从最大自然的角度去，理解它的对，然后然后如果你认为他差不多在做自然，自然的话，其实它和呃子回归其实真的很像啊，或者很相关，然后或者说之前有很多人说什么压缩及智能，其实从信息论角度。

各个角度去理解，它都有压缩的这个感觉，你做做压私人估计都是在压缩，所以所以然后包括就是我们也做一些，transformer和diffusion的结合，如果你相信啊，skating的这个能力。

有可能来自于transformer的话，所以这个事情说不好，然后但是这是数学，那纯数学，所以我怎么也想不明白，为什么比如说视觉的模型现在都这么小，做不大，对不对，为什么。

就比如说像基于diffusion那些像像文道图，我们都是在10亿左右，然后扩大之后效果不明显，所以它解释不了这个问题，它它一定是一个哦，我觉得跟数据或者跟工程或者是跟呃。

我不知道一些knowledge就很相关啊，就就就然后从这个角度去想，你的其实变量很多，它不光是diffusion和呃这个自回归的变量，其实还有语言和图像，它本身模态的这个变量对。

然后我们呃呃最近有些尝试啊，然后我除了刚才智杰提到basin flow network，它是一个连续化的这个diffusion之外，我们也做一些mask discution，就是很离散的。

然后但他们都是defusion model去做语言，然后现在有些进展，比如在g p t two大桥上，其实没有比传送门差的多，所以在它它可能是一个skating，开始探索的起点啊，所以从这个角度来讲。

可能还有很多的这个可研究的空间啊，也有可能是一个很负面的，就是发现语言上你也不能skin up，那就是那那就是自富贵了，对不对，那就那就是自富贵了，对但但但也不是很确定啊，对啊对好我。

我基本上就是有一些这种看法啊，对好的，顾老师有什么观点对我，我对我也谈一下我的看法，呃其实跟我今天talk讲的一样，我觉得这个什么范式其实不重要，根本问题是我怎么做信号拆分是吧。

当我把language你拆分成一个一个token的时候，它天然就适合用auto aggressive model来做，当你把这个image拆分成了这个noise intensity的。

这个每个肯定向这个噪声更小一点的情况下，你天然就应该用diffusion来做，因为这个你每一种这个拆分方式，你其实会带来自己的inductive bias，然后你每一个自己的这个范式。

其实也有自己的inductive BIOS，当你的这个范式的inductive BIOS，跟你的拆分方式的index tive BIOS一致的时候，那你自然就有些好的结果。

所以所以我觉得这个呃个人的看法，我觉得这个问题其实不是那么重要的问题，重要的问题是我们真的想清楚了，怎么拆分，自然而然就会有更更合适的这个办事的出现，对谢谢对我感，我感觉这个郭郭老师的这个看法。

也是比比较犀利对，就是呃确实是这样，是有INDUCTIVICE，包括我想就是像姜老师VR做的很成功，是不是也有这种刚才也讲了，对这种这个视觉的层次化的，这种indux space的考虑。

所以所以所以或许这是一个指引，下一步的这种方向好，那我们的第二个问题可能探讨一下skin law，就是你们刚刚的报告都都出现过skin la，对我我一直有一个特别想不明白的事情。

比如说我们的语言它在scale up的时候，大家现在对他的能力的提升是有一个刻画，比如说它增加了这种推理的能力，增加了word knowledge，增加了比如说去做数学题的能力。

但是我们比如说这个视觉的模型，不管是图的还是视频的，它这个模型大了以后，确实生成的图片质量变好了，FID也变好了，但是就是呃这这具体发生了什么，到底大的比小的是强在哪了，怎么去进更进一步的去刻画。

这个他的能力的提升，以及有什么问题是scaling可能解决不了的，就这这个问题想跟几位嘉宾探讨一下，对啊，我想想我们这菜按什么顺序来对对，我来反过来好嗯对好行，没问题，这个我我我上来就是暴论了。

我觉得这个呃，我觉得是呃，这个scaling law还和和刚才那个还不太一样，skin law是基于这个范式来存在的，就是我，所以所以我猜这个我们现在主要想讨论一下。

diffusion model相关的斯SK领了，对我觉得这个diffusion model没有skalling out，只有一个原因是大家没有一个好的evaluation，Metric。

我们来想想language model为什么有valuation呃，language model为什么有这个skin law，是因为这个有有一个很天才的人，站出来说这个压缩机智能。

所以我们用这个n l l loss是吧，Negative log likelihood，我们就能衡量你的这个language model。

training这个inference以外的这个performance是吧，然后呢他又有这个等价性，哎所以更好了，我们直接把这个SUBTASK直接做一个average，哎我的这个NL了。

这个negative log likelihood的loss，就可以就可以用来度量我们的这个performance，到底怎么样，到defasion model，这这个出问题了，你你没有办法度量。

你有啥办法度量，第一就是我，我今天也提到这个，这个如果你按diffusion的方式做拆分，你的这个importance，你其实是不中不知道的，你不知道到底谁比较重要，所以你不知道怎么把这个每个每每个。

SUBTASK的这个negative log likelihood来进行加权，所以这个首先这边这条路我记了第二条路，这个你用什么，你用FIDFID的问题就更多了是吧。

你首先你你FID你到底用什么抽feature，你抽feature带来多大的BIOS，第二你FID做的这个高斯假设是吧，你这个这个这个那么那么复杂的数据分布，你真的能满足高斯假设吗，你这个度量一阶局二阶。

举这个这个这个这个说出来都好笑是吧，所以所以我我都不知道到底用什么，这个evaluation metric，所以我我觉得主要问题在这，当我们能把evaluation metric搞定。

这些问题其实就就就迎刃而解了，对对您的那个个人简介里面写着，您做那个evaluation那个是啥呀，是跟这个有关的吗，这个没有没有关系，对我我我自己其实也在做一些这个呃，生生成模型的一些这个质量评估。

相关的一些事情，对对这些事情我都挺感兴趣的，对我我这个也也不代表我研究出来了是吧啊，明白明白，好的好的好，李老师有什么看法，好这个对对，其实我我我这个这个问题，还还挺超出我的知识范围的。

因为我其实接触大的很真的很少，就我可能做的都非常非常小，或者是在在大的上面做一些加速啊，之类这种很前期的工作，呃，我自己是这样去感受的，就是从呃你你如果从生成的这个任务上来讲哈，就是其实理想上。

它是有一个非常非常好的生成模型啊，它能够去呃一方面达到最好的like you，另外一方面同时达到最好的视觉效果，或者人的感知啊，这个是有的，然后但是确实有evaluation的很多问题。

导致我们没有办法去量化，它到底走到了什么程度，但但从那个我我我我从，比如从总喜欢从概率角度去讲的话，每一个模型，不管包括干或者包括之前我们淘汰的一些模型，它其实都是一致的，一致是什么意思。

就是如果有无穷的数据，有无穷的优化的能力和无穷大的模型，它原理上就是会收敛到ground true state distribution，对不对，所以从这个角度来讲，从那个极端上来讲。

大家都有skinning了，对的，所以我们关心的是什么呢，关心的是在我们现在的这个资源下，或未来一段资源的情况下，他大家的这个la的曲线，或者所谓的是是超线性还是什么，线性或者次线性是什么样的一个性质。

然后然后这种情况下，就就跟其古老师刚才说的很有关系，就是你从你换metric，你的law就可能不一样，对不对，然后你你你考虑不同的这种贝塔的这种啊，某杂的题，它可能唠又不一样啊，所以所以从这个角度来讲。

就它会非常非常复杂，然后我们imperial里，现在感受就是自回归的这个特性是最好的对，所以这就是我的这种看法，然后但是呢呃怎么才能就是是不是这就是对的，也有可能就是就是这样的啊。

然后或者说如果没有的话，我们怎么发现，比如说diffusion有可能哪个地方没做对，或其他的还有更新的，那那这个就我觉得就很困难，就是我也没有一些什么呃，一些好的这种指导。

能够让我去意识到是什么样的东西，但我确实同意很多古老师这种看法，就是所谓的skin law，它会和metric有很大关系，然后和我们现在处的这个所谓的data的regime。

就比如说或者model size的regime，比如说是在百亿到千亿之前或怎么样的，这个有很大的关系，它它是一个应该是一个非常非常复杂的，然后我们empirical里只能看到的一个。

非常非常狭隘的一个地方，对然后大家也不太会有钱的人，也不会太去乐于去探索其他的，他会强化自强化我们对子回归的这种印象，因为大家会从这个角度去去做，比较安全对啊，这就是我的看法。

啊呃刚谷老师和李老师分享的，其实我有有几个点我是特别的同意，就是从MATRIC呀，还有从一些模态的一些角度，其实嗯是这样，那就是我个人其实在这一块呢，没有特别多的理解。

就DEPUSION的skin nor，但是我有一些这个经验性的这个呃看法，然后呢也想听听各位各位的这个呃想法，就是我发现可能是不是图像的这个defence model。

或者是文道图的defence model上，它的这个呃skinning nore里面啊，我的这个training flops是最有用的啊，就是不是说狂加图片就有用，而是说我要加一些。

就用一些高质量图片好像就够了，然后狂加training flos，然后对吧，然后是不是呃这个我现在也比较关心，就是这个比方说文到视频这样的一个场景里面，那你嗯这个它里面，我们到底嗯想把性能拉上去啊。

比方说就是人的视觉性能，而最后人的观感，那我们到底应该在哪些方面投注呃，倾注比较大的这个呃呃资源啊，到底是标数据呢，还是嗯搞显卡，来来来来串，其实这这些我我也一直在思考，我我也我也不太懂啊，可以嗯。

如果后面几位老师有想法的话，也可以啊，帮我解答解答，我就这些，谢谢，嗯呃我我说一下我看法呃，就是前面几位老师说已经表比较多了啊，包括那个呃metric对，然后我说下我看法。

就是说我认为所有的生成和理解任务，都是围绕着人的，就所有的任务都是人定义出来的，对包括就是生成和理解，包括视频中的，所以我认为语义是最重要的，那语义怎么来呢，就是轮廓还是通过人来。

所以我认为啊diffusion model的PRETRADING，它实际上语义相对呃，呃就是语言模型要差很多对，所以我认为语言模型的语义才是未来，所以我觉得呃我我我的看法就是，嗯语义是最关键的。

如如果diffusion，未来如果能够啊formulation出一套PRE，就PRETRAINING这个语义的这套框架的话，也许它将来会是一个比较完美的框架，但就现在而言的话，就是呃。

就是如果围绕着我们这些人的所谓的一些概念，或者视频中的一些场景，包括文字渲染，包括一些相关的一些task，这些都是最关键的，然后这些语义才是最重要的，对这是我看法，老师好，我来谈谈我的认识啊。

其实我部分观点跟李老师很认很认同啊，呃因为我做多模态做了好久啊，做了45年，我是觉得语言跟就是视觉的这两个东西，差别其实特别大，我我成天还在死磕他们之间的差别，你比如说我最近做了很多事啊。

嗯就是比如说嗯我们的大大模型啊，嗯把语言的那MOE啊，就是套到多模态这边就会发现它的效果很差，其实其实很差的对，很多时候就是因为这个模态变化导致的，你看起来好像token化是一样的，案例的观点都一样。

但是它就是不一样啊，然后比如说嗯你语言模型的幻觉我觉得还好吧，但是在视觉这边他又特别严重，就导致我们很多时候，比如说要把那样多么的大模型，用到跟摄像头结合呀，他是在一些比较严肃的场景。

你的识别率不准或者幻觉比较严重，别人根本不会让你去试的，所以呃所以我觉得这个视觉，这个模特是特别奇怪的一个模态啊，我我我也懵懵懂懂有一点感觉，但是我也说不清楚，我是觉得吧呃语言我是觉得他是人定义出来的。

它是个离散的，甚至我觉得不客气的说，基本上可以穷尽呃，但是好像视觉吧它是个连续的嗯，如果从比如说假设背后有个物理的东西，能表示清楚，它可能能用一个物理的模型表示出来，所以他需要的参数量可能不需要那么大。

但是呢你这么理解，好像觉得SSKINNER不是那么严重的个问题啊，但实际上发现你做在应用的过程中间，又发现，视觉这个东西特别容易出现各种极端的情况，就是啊可令呃。

就是他的那种什么bad case又特别多，导致应用各种困难吧，所以我也不知道，就是朦朦胧胧感觉到了，他可能跟落在这个视觉模态上，好像不是那么严重，但是好像把它做好，我也不知道怎么做好。

比如说好像做设计的人特别讨论吧，比如说VIT那个模型，他们把它加大好像没有太大的意义，就是没有表现出很好的效果来，就是说你可能费了很多事，他提升很有限，这个原因到现在也没搞清楚，这是我的经验哦。

对是不是结论又是不是结论，对我感觉刚才的一些看法都很有启发性，对我自己也朦朦胧胧懂，当然当然我自己也没真正做过大的skill up，但是从大家模型的结果来看，我觉得好像呃就能力的提升还是有一些刻画。

比如说当年做干的时候，大家可能在追求这种呃一些consistency，比如说你生成一只狗，这只狗到底是不是两只眼，那它或者是它有八只眼，就当当时大家是做不到的，后来包括那个那个SD3。

大家看到了一大堆那个奇怪的case，就他他理不理解这个事儿该长什么样，然后到最近这些视频的模型，大家开始谈论说他的一个什么物物理规律，他知不知道这个物理规律长长什么样。

然后呃今天我又看到了那个那个叫dream dream是吗，是啊，那个是刚开完那个视频生成，然后说他那个呃他那个合影，然后OpenAI的那个几个人的合影，然后他变成视频以后，这合影这里的人就开始打架。

然后说这个这个社会学规律，就我们说如果他说作为一个word model的话，他可能会有一些理解，就是说这几个人前一秒还在微笑合影，下一秒他该不该打架，就是嗯我感觉这3号还是能能有一些客观。

你好像观察到就是sell up以后，他3号他他能学到，然后这个幻幻觉，我总感觉可能有可能是大语言模型，scp up也不一定能解决的问题，但你说这个视频里面，他或者图像里面他有没有幻觉，就比如说对。

这就是可能人的这个期待，就比如说你大语言模型，你问他中国这个球队是哪年得到世界杯冠军的，然后详细描述这是怎么夺冠的过程呢，如果给你输出一个夺冠过程，你说这个是不好的，这是事实性的错误。

然后但是你这个视频模型，你全生成一些什么宇航员，骑着马，反而大家觉得这是好的，就其实我感觉这其实也是幻幻觉了，对吧嗯对来做一个，对我就简单的就插一句哈，我感觉幻觉这个事情可能呃，就是伴随着这种概率的。

他就与生俱来的，就就相当于就是说你给他一个什么样的一个，条件的输入，哪怕他没见过，他也要把一的概率分配到后边的所有token上，对不对，所以从这种情况下，他他就一定会有这种啊不好的这种情况出现。

包括我们早期机器学习理论，不也是这个近似概率正确吗，所以从这个角度来讲，你你扩大会缓解你的幻觉，但是它它可能还是会存在，所以可能会有一些比卢老师提到，有些严肃的场场合下，我们可能还是很难去用。

所以即使是图或者视频也也有这种情况，但感觉这还是个比较重要的问题，对对对，是的是的，感觉现在这个方面研究其实还是比较少，对好我们按预设的这个时间还有10分钟，我还准备了一个问题，但是我我也想听一下对。

就是台下有没有什么问题，没有，有没有人能帮忙递个话筒，或者我来递吧，嗯其实就像老师们刚才提到这个幻觉问题，然后我想问一下说可不可以利用说思维链，然后去嗯的利用提供一些结构化的推理路径。

然后引导模型产生一些扎实和实质性的输出，就是刚才提到这个幻觉效应，然后去利用思维链，然后去引导模型，产生一些扎实和事事实性的输出，包括这种有没有一些就是说方向的应用，简单回答下就行。

这其实我也不认识这方面专家，但应该是有相关的工作，那个在benchmark上能证明他把就是通过思维链，或者是一些什么prompt的手段，可以缓解那个呃缓解这个幻觉的问题。

然后我记得我我们我们上海交通大学，清源剧院的那个刘鹏飞教授，他也做了这个事实性检查的一些工具啊，也就是说借助这些辅助的外外部工具，我觉得幻觉肯定是一定程度上，我们很很很多可以这个消除掉的，对对是的。

谢谢我多多说一句哈，我感觉就是你思维链这些，它其实相当于就是呃输入不同的条件，然后呢呃让他去用对应的条件模型来回答，你虽然都是一个模型，但是你的prompt这个条件的改变。

会让你后边的这个条件概率有一个显著的改变，所以它会有区别，然后呢，你合适的prompt，可能会激发他合适的语料的能力，比如说什么，我之前上课经常举例子啊，就比如说谢邀，人在美国刚下飞机。

如果你输入这个problem，他显然会用知乎的语料，去相关的这种东西去回答，它会激活那个地方，然后你用一些其他的可能激活其他的，所以这会有区别，然后呢但它就是它可能会在某些特定的任务上。

你用特定prompt，它应该会有好的表现，但sam how呃，未见得会有一个universal的好的一个东西，他一定会对所有的任务都会好啊，这可能是比较困难的，对对对，但但对。

但anyway就是很多现在这种探索，我觉得也都是有益的，比如思维链啊，或其他的一些对，当然可能我们还缺乏一些，就是呃呃更深的理解吧，对对好，也也谢谢这个问题啊，还有没有什么呃嗯听众们想讨论的问题好。

我们还有几分钟时间，我们再稍微简单的讨论一下吧，反正因为我们做研究，我们还是想关注这个有什么问题还没有被解决，所以可能现在比较哦有一个哦好好好，哎我我想我们现在实际上嗯，在研究这个问题当中。

我们肯定是没有你们那么高端，也没有你们那么基层，但是国际的应用层在十几年前，我们已经在筹划怎么样要结合思维念，写出一篇授课稿，是我要给大家讲一个讲一堂的课，思想政治教育课，那么那个稿子我以往有模板。

有十几20篇，那个好的模板了，有些是讲故事的，有些是跟讲道理的，有些是举数字来跟你说话啊，说话的一我的性格是什么，我这个人就喜欢讲故事，我这个人最喜欢讲哲学，那么你就选你的模板么，选了模板以后。

但是呢最新的事例还要根据我的思维念来来走，视力要换，不断的要换，那这里呢我在半个小时之内，人机结合人机互动，我要写出一篇很有就说感染力的授课岗，这个思维链呢要是跟着我走的，它不论是说你这个大模型。

你想我这几个条件除了什么就是什么，如果我们现在就说，这几天我们已经进行了一些训练，但是大模型那个可能还还达不到，但是呢就是他怎么样跟着我结合起来，哎就这个，这个属于您，您的这个算是一个呃不问题。

我现在就是如果是有需要，我们马上肯定就能课项目就能立起来，这个问题是说怎么做这种，这个人在环路的这种生成，是是我不知道我理解的是正确的吗，嗯反正这个问题，我可能就说现在大模型它写出来的东西啊。

就说大语言模型写出来的东西，一说多一幅画，写一篇小说，它没有标准，但是我要讲课的时候，我是部队出来的，我还是在认为我就是讲思想政治教育课的时候，他现在这个信息瞬息万变了，变得很多了。

人比大脑他处理不了这么多信息了，我们最早给下面讲课的时候，我是占据信息的绝对的主动权，我拿一张参考消息，他们有，我就可以跟他讲，现在下面听课的都比我掌握的信息要快了，那么这么远，我就要怎么样迅速地。

按照原来一些好的讲课稿装，说，说白了就是旧瓶装新酒装进去，在我喜欢的模板之下写出一篇稿子出来，感感感谢您，您您我您，您用一句话能不能提一个问题，谢谢谢您一句话，谢谢你提提一个问题。

就是您一句话总结一下您的问题，然后我们方便我们理解，然后我们是不是在回答还是什么对，一句话就说就是要跟着我的思维，去协助我写出一篇很生动的思想，政治教育授课稿，我我总觉得这是一个长上下文的问题。

就是呃我想着那个之前有一个叫模世界模拟的，那么一个英勇，说一堆那个小人儿，他们在一个小镇里面，然后让让他们去那个互动对吧，然后呃每个人他有自己的记忆，他他记得自己说过什么话，然后他跟别人说过什么话。

他自己的背景是什么样，然后呃他这样的话，它就能生成的更加这个符合这个人的特点，然后我总觉得这位老师的问题，可能是一个长上下文的问题。

就是如果我们有infinite lg的这个context length，那我就能把你对吧，你出生以来，所有的这个哼发生跟世界发生交互的内容，都记下来，放到你的长上下上下文里面，然后再再去记忆这个生成。

它可能就是像你说的这个什么呃，符符合要求的这种思想政治课，对之之前好像有一个那个数字分身，我不知道是那个那个是谁的CEO，跟自己数字分身对话，是一个个性化的GPT，把他的所有采访访谈。

写过的书都输给那个GPT训一下，然后他就来来模仿自己，就跟自己对话，好像有一个那个video可以可以可以可以看一看，但我忘记叫什么名字了啊，对对对对好，那当然了，还有几个更深层的，我不帮平那手。

我下来以后，我还可以再跟你们讨论几个，更更是我们可以还线下还可以交流，谢谢谢谢，我们对我们还稍微有个35分钟，要不我们再在这个再再跟讨论一个，各位老师好，呃，就是我这边的就是两个问题的话。

我是从应用应用的那个角度来提，提的问题就是第一个的话是嗯，因为我们一直在找一些那个场景嘛，就是现在这个扩散模型的话，嗯就是我们有有这样一种场景，就是想在比如说我呃，我在我要在一件T恤上对吧。

我还要印上一个啊，就说比如说是圆桌讨论这样四个中文字对吧，但是现在的那个呃很多模型的话，我看他是就是就是我们去找找这样的模型，就没有，好像我不知道是可能是我不知道对吧，但是我就想请问老师呃。

国外老师就是我如果要达到这样一个效果，我是已经有现成的这个技术了，还是说有什么途径去呃，找到这样的一个呃这样这样的一个呃模型对吧，嗯这是第一个问题，您您能再重复一下要达到什么样的效果，但啊就是很简单。

比如说呃我要在我的这个，我我说说出一个提示对吧，我要在我这个T学上啊，打印出呃呃呃圆桌讨论四个字啊，这个扩散模型里面就是那个，比如说SSDX2嘛，对不对哦，这这这文文本生成文本线染哦，纹身图对。

就理解成纹身图啊哈哈，因为啊因为就是我看现在最新出那个SD是呃，三对吧，它其实是可以支持呃，比如英文喊我对这支持的挺好，但是中文其实是没法支持的，然后在chat g p呢。

它其实也基本上是不支持这个东西的，对其实其实大力三做的真的很不错，那个现在stable diffusion3，这个相对于SDXL在这方面提升也很多，然后呃我们我们有些同事在这边。

其实也做过一些很不错的一些探索，但他们做的是这个专有化模型，大部分走的到技术路线是这个有一个这个foundation model。

然后在foundation model的基础上做post training，就拿这些字专门做post training，然后将train的这个专业化模型，其实做的结果还可以啊。

就是要自己训练个专业模型是吧，是的啊，就是不能在现在现在这个这个呃，开源的预训练模型上其实是达不到的对吧，开源有的预训练模型也还可以啊，比如stable depension3也还不错呃。

但是中文是我试了一下，好像是不行的，因为他刚不是开源出来了吗，我不知道各位老师有没有试试过这个问题，对我我其实没有试过中文那个呃，但也可以理解嘛，这个中文数据量毕竟相对少一些，yes8C一些。

这是因为呃是呃他训练的时候，这个语调的问题是吧，对我觉得是的啊，谢谢，这还有一个问题的话是呃就是现在这个SD的，他对这个就是位置是不是也不是太敏感，比如说我想在这个呃一个呃一张图片。

就说我想生成一张左下角对吧，我左下角生成呃啊，就说生成一只小狗，对不对啊，就是我的提示语对吧，我左下左下角生成一只可爱的小狗，那这个的话基本上我试了一下呃，也很就是现在这个这这些模型基本上都呃。

很难做到这个事情，那这个的话是有没有什么解决的一个途径啊，对那我回答一下那个对我，我觉得我觉得这些这个foundation model之所以做不好的，原因还是因为数据量的欠缺。

他们的这个decomposition double，不好意思说错，他们的这个disc entanglement做的不够好，所以针对这些问题，我我自己认为general的这个最最简单实用的解法。

就是做post training，你不管是用RL的方式做，还是用这个其他类似SFT的方式来做，就collect这B系列data，然后这个对着它在to就好了啊，就是还是就是去训练自己的专业模型是吧。

对是的哦好的，就是现在是这个就是开源的好，这这这这方面有限，有做的比较好的吗，还是说都是还是得自己去训练，因为这个我觉得它其实是一个通用的，一个应该是个通用的需求，对吧啊，还不是说一个行业。

说专业行业的需求还是什么的，这个对，因为因为这个这个，这个其实其实这个问题其实很多，这个你说这个position也是问题，你说这个一些什么一致性也是问题，然后包括您说的这个字体都是问题对。

就是一个general模型，这个拟合不了这么多复杂的数据分布，其实也是可以理解的，因为你看像我们做language model，其实也不是这个真的一个模型，他就把everything都做好了。

有人这个针对读读论文去做了一些fine too，做针对这个其他一些task做了一些发音to，我觉得在目前阶段是可以理解的对，所以目前阶段呢我觉得这个做一些specific model。

是一些相对更靠谱的路，对啊好的，谢谢好，我看我们的时间也差不多了。