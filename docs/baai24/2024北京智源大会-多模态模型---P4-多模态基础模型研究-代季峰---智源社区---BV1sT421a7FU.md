# 2024北京智源大会-多模态模型 - P4：多模态基础模型研究-代季峰 - 智源社区 - BV1sT421a7FU

哈喽哈喽哎能听见哈，好，那个非常高兴今天能够来做这样的一个分享啊。

![](img/cd36720746771989945984d85ff95f1d_1.png)

我今天报告题目叫做多模态技术模型技术研究。

![](img/cd36720746771989945984d85ff95f1d_3.png)

对然后首首先呢我们可以看见，就说当这个线已经是有共识了哈，就说大圆模模型呢，它它已经带来了非常大的一个改变啊，它带来这么大的一个改变，我觉得是来来自于两个方面，第一个方面呢它在具体的语言相关的任务上。

它的性能做的非常好了，通过skating low啊，但另外还有一点很重要的呢，是它的这个通用性就说它降低了呃，应对新的开放式任务的这个边际成本，这点其实是非常重要的，就像上一代的AI啊。

比如说以前这个对吧，上一代的AI创业公司做安防等等等等的啊，比如说你给一个城市啊，部署了一套安防的系统啊，我们要这个评全国卫生先进城市，所以接下来我们要严抓这个乱扔垃圾的行为。

所以呢我们需要你们给我们再部署一套，这个检测乱扔垃圾行为的这样一个系统啊，那你吭哧吭哧，你又得花十个研究员采集10万张图片，100张卡啊，然后再干个两个月啊，然后再给他交交付这么一套系统。

你应对任何一个新的任务，你的边际成本是很高的啊，但但是像以CHRGBT为代表的这种大圆模模，模大圆模型啊，它有很强的应对开放式的任务的能力啊，你比如说你在网上CHRTPT。

你响应这个上亿人的这个请求的时候，他并没有说open AI在给你，另外花研究员在帮帮你再发一痛，for for你的每一个任务，对吧啊，就是这样的一种呃，一个是呃应对具体的任务能力非常强。

另外一点呢就是开放式的这个任务的能力低，边际成本泛化的能力，带来巨大的一个生产力的一个变革，当然呢我们就想把这样的一种新更更更更新的，一种生产力的能力啊，带到更多的这种多模态的领域里面去。

因为这个世界并不是这个结构化的语言，世界就获取更多的信息，你需要通过视觉，然后以及你需要主动的去跟现实环境去打打交，打交道啊，就是呃这样的一种多模态的能力，会是如果我们也具有，就说是这样的一种啊。

每个任务能力很强，以及这种开放式的应对各种任务的能力，那会带来一个巨大的一个生产力的一个变革啊，所以呢啊我我们就就就就开，以这样的一个为出发点呢，我们就开启了我们的多模态的，基础模型的技术的一个研究。

对，然后呢我会具体的讲一讲啊，当然我这这页PPT是按照国内的这这这种高，高校里面的习惯做的这个PPT，然后在在在在场的，今天的333位，都都是比较这个西方学术这种风格，所以这个PPT风格会有点不太一样。

对我们先先先看一下这个呃，呃呃就是第一个主要的挑，主要的挑战就就是关关，关于就是预训练以及训练数据规模的问题对，然后呢，嗯哎这个确确确实这个软软件不太一样啊。



![](img/cd36720746771989945984d85ff95f1d_5.png)

但没关系，并不影响这的理解对，就是说我们手术。

![](img/cd36720746771989945984d85ff95f1d_7.png)

我们首我们首首首先关注的第一个问题呢，我们会会发现就是呃。

![](img/cd36720746771989945984d85ff95f1d_9.png)

当然之前的各位讲讲讲者都有说到clip啊，就是说呃已有的这。

![](img/cd36720746771989945984d85ff95f1d_11.png)

这这这这种就是图像编码器的这个预训练，通常都是用clip这样的。

![](img/cd36720746771989945984d85ff95f1d_13.png)

在图文成对的数据上去进行训练的啊，但是呢就当我们真正要构造一个呃。

![](img/cd36720746771989945984d85ff95f1d_15.png)

非常大规模的这个多模态的基础模型的时候。

![](img/cd36720746771989945984d85ff95f1d_17.png)

我们会发现现在其实互联网上的图文成对数据，就算你都用光啊。

![](img/cd36720746771989945984d85ff95f1d_19.png)

它的数量也是已经告急了啊，就是说这个他已经无法再支撑更大规模的这个。

![](img/cd36720746771989945984d85ff95f1d_21.png)

多模态的大模型的这个预训练，然后以及呢它还有一个很重要的一个问题呢。

![](img/cd36720746771989945984d85ff95f1d_23.png)

就说是图文成对数据里面，你你你可以你可以想象。

![](img/cd36720746771989945984d85ff95f1d_25.png)

就比如在一整整个一个复杂的一个新闻，或者在一个文档里面啊。

![](img/cd36720746771989945984d85ff95f1d_27.png)

然后其实图文成对的部分，那个其实是它里面中间非常少的一部分。

![](img/cd36720746771989945984d85ff95f1d_29.png)

然后以及呢它的这种呃语言部分，它其实是非常薄非常薄弱的啊。

![](img/cd36720746771989945984d85ff95f1d_31.png)

整个一个一个新闻，你的captain什么的，其实只有1。1。11。1点点的文字啊。

![](img/cd36720746771989945984d85ff95f1d_33.png)

他其实很很多大段的逻辑对图的阐释什么的。

![](img/cd36720746771989945984d85ff95f1d_35.png)

相关的东西其实都是在他正文里面，所以呢啊就就就就说。

![](img/cd36720746771989945984d85ff95f1d_37.png)

如果你纯粹只在图文成对的上上，上面去训一个多模态模型的话，其实你的语言模型的部分是会训毁掉的啊。

![](img/cd36720746771989945984d85ff95f1d_39.png)

对对，所以呢就说我我们就我单单就说呃，我们这边呢就说是我们在在在训这个，比如说上海人工智能实验室这边的，我们的这个多模态大模型的时候，我们就会想到原生的，就从这种图文教交错数据出发啊。

然后来进行这样的一个探索和一个训练啊，因为就是图文成成对的部分，只是比如说你那个，我不知道这个怎么样对就是图文成对的部分，只是在中间非常少的一部分，我们希望用到更加广阔的这个数据，还有这个知知识。

来训练我们的这个多模态的模型啊，啊为此呢，我们构构造了现现在开源的最最大的这样的一，个图文交错的这样的一个呃数据集啊，可以看到呢，这呢是跟现呃，是跟以前的这样的一个呃，开源的这种数据集的一个比较啊。

以前当然有有有有很多的是一个纯粹文字的，这样的一种数据集啊，你有很多的文字，但但是你并没有语言呃，呃不但但是你并并你并没有图像，然后我们有比如说像LIN2B啊，这样的图文成对的这样的这种这种数据集啊。

它里面有有一些图图图图像啊，还还算挺多的图像，两个bin啊，但是他的语言部分其实是非常薄弱的啊，然后呢呃我们构建了这个数据集呢，叫做OMONLYCOPUS啊，然后这个是清华还跟跟跟上海人工智能实验室。

大家一起打造的，对它以中英文为主的这样的一个语言，然后呢它有非常大量的这个图像啊，因为它可以更加兼容并蓄的这个收集，更多的互联网上的这样的一些素材，然后以及呢它有这个也是海量的这样的呃。

文本的这样的一个素材，它是现在国际规模最大的图文交错的数据集啊，这个呢是里面的一些例子啊，对可以看到就是它整个主要的这种数数据，就是这样的一种结构对，然后这个地方是这个项目的二维码。

嗯接下来呢我们就就就就就说怎么样，能够有效地利用这种互联网尺度的图文交交，交错数据进行一个预训练，对以前的这种，刚才各位嘉宾都有讲到的这个呃clip的方法哦，哇这个小小小小小绿点在上面压根就看不见。

对A图中间的这种嗯基于clip的方法方法呢，它设计来是利用这种图文成对的数据的啊，它能够在图文成对的数据上很好的工作，去非常好的这个有监督的啊，预训练，你的这个就就就是微信的这个encoder啊。

然后呢，嗯然后呢，以及托以及以前呢有有有C这样的一种方法啊，C这样的方法就是以前确实有有有工作啊，尝试去利用这种图文交错的数据啊，但是呢它其实它的出发点啊，并并不是说怎怎么样。

我能够很好的利用这些非常大规模，图文交错数据，我把我的微信的encoder也训好，然后把我我我拉我我language部分的decoder也也训好，他的出发点呢啊，其实或者说他他能够做到的事情呢。

是说是我拿一个已经预训练好的clip，比如说clip，尤其是clip已经预训练好的vision的encoder啊，我拿过来，然后我再拿过来一个已经预训练好的，一个language的model。

我把它们拼接起来，然后然后呢，我这个时候呢，我用一些图文交错的数据进行试训啊，最后达到一个呃呃部署起来效果还不错的，这样的一个图文多模态的这样的一个模型，它并并它并不是说是诶，我想从头把。

我怎么去更好的去利用图文交错的数据，去预训练我的这个vision的encoder啊，就他他他他并并不能够做到这样的一件事情，因为我们发会发现他会把这件事情给做毁掉，就说如果从头开始训的话。

然后我们这个方法呢B这个方这个方法呢啊，我们待会会会讲，我们提出来叫多模态信息压缩，学学学习这样的一种方式啊，我们首次支持了互联网尺度的图文交错数据的，端到端的啊预训练的算法啊，它可以直接利用。

就说图文交错的这种数据，互联网尺度的，然后from scratch，把你的微信encoder把把把它给训出来，把它给训好，然后后吃支持了我们现现现在最强的这个，视觉语言啊，技术模型。

INTERVL的这个V1。5版本的这个训练，然后项目的那个二维码是在这个地方啊，然后这个多模态信信息压压缩学习这个算法呢，啊我们是从语言这这这这边的这种压缩学。



![](img/cd36720746771989945984d85ff95f1d_41.png)

学学习这样一种概念来的对，就就就说这个最最最近有一种观点。

![](img/cd36720746771989945984d85ff95f1d_43.png)

非常受到大家的这个关注吧，啊就就就说他尝试去解释。

![](img/cd36720746771989945984d85ff95f1d_45.png)

为什么像GPT这样的语言模型。

![](img/cd36720746771989945984d85ff95f1d_47.png)

它能够具有类似于像AGI的这种能力啊，就所谓的压缩及制冷啊。

![](img/cd36720746771989945984d85ff95f1d_49.png)

你把全世界的这种语料的这种知识，压缩到一个呃一个参数量有限的。

![](img/cd36720746771989945984d85ff95f1d_51.png)

这种大模大模型里面啊，在这这样的一个信息压缩的一个过程中间。

![](img/cd36720746771989945984d85ff95f1d_53.png)

自产产产生了制冷啊，然后呢我们也是非常bin这样的一种想法。

![](img/cd36720746771989945984d85ff95f1d_55.png)

所以呢我们在做这个呃，图文交错的这个预训练的时候啊。

![](img/cd36720746771989945984d85ff95f1d_57.png)

我们也是基于这样的一个多模态的信息压缩。

![](img/cd36720746771989945984d85ff95f1d_59.png)

学学学习这样一个理念出发，来构造我们的这个图文交错数据的。

![](img/cd36720746771989945984d85ff95f1d_61.png)

这样一种预训练的算法啊，只是说呢在这个里面呢，啊啊我我就不不不讲那些公式了哈。

![](img/cd36720746771989945984d85ff95f1d_63.png)

我讲讲一讲背背背后的一些想法对，然后就只是在这个里面呢。

![](img/cd36720746771989945984d85ff95f1d_65.png)

在图文教交错数据里面，它跟语言模模型这边有一点很很大的不一样的。

![](img/cd36720746771989945984d85ff95f1d_67.png)

就是语言这边它是一个结构化的啊，这样的一个数据啊。

![](img/cd36720746771989945984d85ff95f1d_69.png)

所所所所，所所以呢你只管压缩你所有的语料就好了。

![](img/cd36720746771989945984d85ff95f1d_71.png)

它已经过滤掉了现实世界中间很多的噪噪音啊。

![](img/cd36720746771989945984d85ff95f1d_73.png)

它已经是人类啊，就是说人类的知识的这个结结晶啊。

![](img/cd36720746771989945984d85ff95f1d_75.png)

把把把这个繁杂的现实世界，通过语言把它给结构化，逻辑化，把它给屏蔽掉了。

![](img/cd36720746771989945984d85ff95f1d_77.png)

把中间很很多的不相关的信息啊，但但是当你在全世界的这种图文交错数据上面。

![](img/cd36720746771989945984d85ff95f1d_79.png)

去做这个预训练的时候啊。

![](img/cd36720746771989945984d85ff95f1d_81.png)

你的图像在在在这边，其实它中间是有很多无关的这种信息的啊。

![](img/cd36720746771989945984d85ff95f1d_83.png)

就是它有很多的信息，对于你这个训练这个图像的encoder。

![](img/cd36720746771989945984d85ff95f1d_85.png)

或者一个多模态的大模型来来说，这些信息它它是LIS啊，它是不相关的啊。

![](img/cd36720746771989945984d85ff95f1d_87.png)

以relevant啊，或或者说它它它就就就是它是一些无效的。

![](img/cd36720746771989945984d85ff95f1d_89.png)

这个信息，所以呢我们的这个压缩学习compression learning啊。

![](img/cd36720746771989945984d85ff95f1d_91.png)

它是在在图像层面，它是在latent的呃，encoder的这个这个feature上面去做的啊。

![](img/cd36720746771989945984d85ff95f1d_93.png)

我们我们图像这边先先通过一个呃encoder啊。

![](img/cd36720746771989945984d85ff95f1d_95.png)

先呃或获得一个隐变量的一个表示。

![](img/cd36720746771989945984d85ff95f1d_97.png)

然后我们在这个隐变量上面去做这个，compression learning啊，这样呢可以就是通过学习过程中间自动的丢弃。



![](img/cd36720746771989945984d85ff95f1d_99.png)

丢弃掉这个这些呃繁杂的世界的图像。

![](img/cd36720746771989945984d85ff95f1d_101.png)

中间跟我们的学习目标不相关的，这样的一个部分。

![](img/cd36720746771989945984d85ff95f1d_103.png)

这样获得一个更好的一个学习的效果，然后最后呢啊这个根根据这样的一种呃。

![](img/cd36720746771989945984d85ff95f1d_105.png)

呃呃latent variable的compression learning啊。

![](img/cd36720746771989945984d85ff95f1d_107.png)

这样这样的一种方式呢，我们会导出最后它的整个训练。

![](img/cd36720746771989945984d85ff95f1d_109.png)

整个训练的这个target啊，包包包括了contrast。

![](img/cd36720746771989945984d85ff95f1d_111.png)

CONTRASTIVE的lows，还有auto regressive的这个呃text generation laws啊。



![](img/cd36720746771989945984d85ff95f1d_113.png)

这样的两个部分来组成细节呢，大家可以去查看论文，然后实现起来呢也是非常高效的。

![](img/cd36720746771989945984d85ff95f1d_115.png)

因为我们要在全世界的这个图文，交作数据上面去做训练。

![](img/cd36720746771989945984d85ff95f1d_117.png)

所所以说我们必须要确确保整个算算算法的，这个呃学学学学习的是一。

![](img/cd36720746771989945984d85ff95f1d_119.png)

一个是呃，一个是accuracy啊，另外一个是以EFFENCY啊。

![](img/cd36720746771989945984d85ff95f1d_121.png)

效率我们都得非非常关注，不然的话是做不好的。

![](img/cd36720746771989945984d85ff95f1d_123.png)

Ok，那接下来呢，就就就就说我们用我们这样的这种数据，还有预训练的算法，我们怎么去预训练我们的这个呃视觉，还有图文的这样的一个基础的一个，feature的一个表征，然后呢嗯我们这呢就是我们做做出来的。



![](img/cd36720746771989945984d85ff95f1d_125.png)

就是这个现在最好的开源的这个视觉语言，的基础模型叫做intern v l啊。

![](img/cd36720746771989945984d85ff95f1d_127.png)

嗯我们这个思考这个东西的出发点呢，就就就就说也就是说呃。

![](img/cd36720746771989945984d85ff95f1d_129.png)

我们现行就是你购构造一个图文的这样的一个。

![](img/cd36720746771989945984d85ff95f1d_131.png)

多模态的一个基基础模型，它包括图和文的部部分啊，但现像现在刚刚才那个赛三林也说。

![](img/cd36720746771989945984d85ff95f1d_133.png)

现在这种呃这种架构通常是这样的啊，然后我们会观察到呢。

![](img/cd36720746771989945984d85ff95f1d_135.png)

就vision的encoder的部的部分，其实我们觉就觉得他有落后于这个时代啊。

![](img/cd36720746771989945984d85ff95f1d_137.png)

最早的时候就是是你你为着这个I呃，这个iimage let上的的任务上面去去去围围。

![](img/cd36720746771989945984d85ff95f1d_139.png)

去围着他来转啊。

![](img/cd36720746771989945984d85ff95f1d_141.png)

这样的一种去训练的方式啊，然后呢，呃后来呢又迁移到ECLIP为代表的这样一种。

![](img/cd36720746771989945984d85ff95f1d_143.png)

图文成对的这种CONTRASTIVE啊，预训练的方式啊。

![](img/cd36720746771989945984d85ff95f1d_145.png)

你最后得到的这个VISHENCODER啊，它还还是跟像现在这样的一个多模态大模型。

![](img/cd36720746771989945984d85ff95f1d_147.png)

后面接一个非常强的这个具有可以，你可以认为具有高高阶逻辑和认知。

![](img/cd36720746771989945984d85ff95f1d_149.png)

逻认知能能能能力的大圆模大圆模型啊。

![](img/cd36720746771989945984d85ff95f1d_151.png)

做作为你的这样的一个推理和高阶。

![](img/cd36720746771989945984d85ff95f1d_153.png)

这个智能能力的中枢啊，视觉作为一个encoder，它并不是为这样的一种架构。

![](img/cd36720746771989945984d85ff95f1d_155.png)

一开始它并不是为这样一种架构去设计的啊，所所以呢我我们在这做的这个intern v l。

![](img/cd36720746771989945984d85ff95f1d_157.png)

这整个模型呢，我们其实就说是一开始。

![](img/cd36720746771989945984d85ff95f1d_159.png)

我们就是为整个这个呃图文的这样的一个。

![](img/cd36720746771989945984d85ff95f1d_161.png)

多模态的大模型，就一开始就是为这样的一个应用，或者说这样一个架构去进行设计的啊。

![](img/cd36720746771989945984d85ff95f1d_163.png)

就是我们一开始就想就想想清楚，视觉作为一个很强的这个encoder啊。

![](img/cd36720746771989945984d85ff95f1d_165.png)

然后语言作为一个呃，一个就是高高阶智能的中枢d decoder啊。

![](img/cd36720746771989945984d85ff95f1d_167.png)

就是为这样的一个架构去进行设计的啊。

![](img/cd36720746771989945984d85ff95f1d_169.png)

然后呢我们这里面呢采用了呃这个latent，concompression learning的方方方方法。



![](img/cd36720746771989945984d85ff95f1d_171.png)

来进来训练我们的视觉的这个encoder啊，然后训练了一个非常大规模的线。

![](img/cd36720746771989945984d85ff95f1d_173.png)

应该也是现在最强的，开源的这个视觉的encoder啊，然后以及在中间呢。

![](img/cd36720746771989945984d85ff95f1d_175.png)

我们使用了一种就是叫做渐进式对齐的方案，来进行一个学一个一个学一个学习啊，因为你一开始，如果就如果就就如果，就就就就说接一个特别大的语言语言模型，然后来进进进进行这样一个视觉encoder。



![](img/cd36720746771989945984d85ff95f1d_177.png)

从头从头开始训练的话，你的整个计算代价会是非常高的啊，所所以呢我我我们是在一开始在训练这个视觉，encoder的时候，我们是为啥我不能我哦，我们在一开始训练这个视觉encoder的时候。



![](img/cd36720746771989945984d85ff95f1d_179.png)

我们是用一个相对比较小规模的语言模，语言模型。

![](img/cd36720746771989945984d85ff95f1d_181.png)

在图文交交错数据上进行这个呃，视觉encoder与训练啊，然后视觉encoder训训的差不多之后，然后我们再换上一个呃，特别大的这样的一个非常强的，这样一个大的语言语言模型，进行进一步的这样一个续训啊。

通过模模型这种从小到大，数据从粗到精的这样一种渐进式的训练策略啊，大幅度的降低了大模型的这样一个训练的，这个呃成本啊，在有限的资源下，展现出来的卓越的这样一个能力啊，然后我们先先先先说这个模型拆出来。

中间的视觉编码器的部分啊，一个6B的一个视觉的encoder的一个模型啊，这个是呃现象是应该是现在最最好的，这个开源的这个视觉的encoder啊，我们的能力呢能够比肩这个谷歌的。

它闭源的VIT22B这样的一个呃性能，然后呢呃作为整体整个呃模整，整体整个模型就是视觉的encoder，加上语言的底decoder啊，这样啊整体整个模整个模型呢，它现在是世界上最强的。

开源的多模态的通用模型啊，性能媲美啊，基于gt4V啊，GEMINI啊，GROCK啊，等头部的这种商用的模型啊，然后呢这个模型2023年12月份发布啊，在哈根face上面呃。

增长趋势榜单上连续第一个月啊排行第一，然后在视觉语言技术模型总下载榜单上，排名前十啊，然后在他旁边的都是google啊，Meta，还有就是呃微软啊啊，更早时间发布的这种同种类型的一个模型，对，然后当啊。

北京志愿研研究院呢有发布这个评测体系啊，然后反正呃是是目前最好的开源模开源模型，然后在呃浦江实验室的到这个思兰品评测上面，它是一个榜单的榜单啊，然后呢他也也是呃就是它是十几个，这个就是多模态模型榜单。

然后他做一个加权求求求求求，和这样形成一个榜单的榜单啊，然后呢他又优于国国国内非常多的啊，像那个质朴，还有街月，还有那个阿里的这个这个闭源的模型，然后呢比我们比我们更更更好呢。

就是这个GGB4V的4月份的版本，以及最新的这个GP4O对啊，这个呢是我们可以跟大家展，展示了我们这些模这个模型的一些视觉内容，理解啊等等能力，然后这个呢是我们那个那个模型的一个。

online的一个demo啊，大家可可以先先存下这个二维码，主要是现现现现现场这么多人，如果同，因为我们那个服务器并并没有放放放很多啊，因为我们其实主主要还是就说是开源，这个weight啊。

这个并并不是拿拿来就说大规模的，这个给给全球这个上亿人服务的，所以我们放放demo的那个机器计算量比较小，如果你们同同时使用的话，我估计那个体验不会那么好，但拼平时你们正常使用的话，这个这个还是非常快。

然后体验也是非常好的对，然后这呢就是更多一些例子，比如说这个里面你问他宝马车牌号是啥，不拉不拉啊，他能给你很好的回答，这个哪个西瓜最熟对吧，我记得好像这个是质朴。



![](img/cd36720746771989945984d85ff95f1d_183.png)

他他们他们的一个例子对，然后呢，Anyway，他也能给给你一个非常reasonable的一个答案。

![](img/cd36720746771989945984d85ff95f1d_185.png)

对这个是我我自自自己加的一个例子，反正就是我们这个模这个模模型做，做出来之后的话，然后我们就就那个做这个模型的同学，大家就有一个微信群嘛，然后模型做出来之后，大家就拿自己这个手手机相册里面的各各种照。

照各种照片去试啊，然后就有一些非常有趣的或者FAURE的case，大家就会在在群里面聊，对，就这种感觉，感觉跟跟跟你去试用一个商用的模型，比如说你不知道它怎么训练的模模型相比，这个体验还是非常不一样的。

对，就说你说你用这个GPT，4V或者是什么的模模型对吧，你觉得它效效果挺好，OK行，那效果挺好，这个后后面是是魔法对吧，open AI的，open AI的magic对吧。

然后你做一个真正自己顺顺的模模型，然后性能当然跟跟它差差差不多，然后你发现一些效果特别好的例子的时候，你会觉得我靠这这些东西是是是怎，是怎么实现的，然后你你会要就比如说这个这个力，这个力例例子里面。

反正就说他能够在这种遮挡，很很严重的情情况下对吧，然后他他能够分分清楚左左右手的概念啊，然后他能够知能够准准确的告诉你答案，然后我自己看这个碟子，我觉得哦忘了对，就在在在在在在我的理解里面呢。

其实就是他就就像刚才刚才赛林，有有有有有讲的，他其实我的理解他这种在全世界种语料上，这种图文的这种语料上面训练之后，他其实是学会了这种呃单位的这种词源，跟这种视觉的单位元素之间，它的一个映射关系啊。

然后他学学学会了这样的一种映射关，映射关系之后呢，然后他又学他又并且它又具为具有一种组合，一种排列组合的能的能力啊，他能够理理解整个句子，把这些各种各样视觉元素排列组合起来之后。

它能够对应于一个什么样的一个，更复杂的一个概念啊，通过这些基础能力的组合呢，它能够形形成一个很强的一个泛化能力，就是我们在我们在在在测试我们模型的时候，会发现诶这个这个模型的能力。

我们没我们没有训练过啊，他是怎他究竟他究竟是怎怎怎么出来的啊，就是经经经常我们都会有这样的一种感慨，对啊，但这次这个是一种指令跟随能力，这是GPT4V，当时让我们觉得很惊艳的一个例子啊。

我们的模型也可以做的对。

![](img/cd36720746771989945984d85ff95f1d_187.png)

然后这次这个是网网上，别别别别别别别人做的一个视频啊，这个不不是。

![](img/cd36720746771989945984d85ff95f1d_189.png)

这个号称是最接近GPT4V的，开源可商用多模态大模型，一开始我也以为是在吹。

![](img/cd36720746771989945984d85ff95f1d_191.png)

因为我们本本身也有图片识别和理解的，实际业务场景，就实测了一下，结果给我的感受就是。

![](img/cd36720746771989945984d85ff95f1d_193.png)

将对传统的OCR形成降维式的颠覆式的打击，可以看一下公有云提供了类似服务。

![](img/cd36720746771989945984d85ff95f1d_195.png)

比如证照识别，票据识别，各种资质的识别等等，价格其实还不便宜。

![](img/cd36720746771989945984d85ff95f1d_197.png)

现在这个多模态的大模型就可以搞定了，我随便找了几张不同类型的图片看一下，效果真的是惊到我。

![](img/cd36720746771989945984d85ff95f1d_199.png)

我上传了一张有点模糊的身份证，让他用JSON的格式返回，可以看到他返回的速度很快。

![](img/cd36720746771989945984d85ff95f1d_201.png)

而且数据很准确，又用一张消费清单试一下，还是用JSON输出。

![](img/cd36720746771989945984d85ff95f1d_203.png)

让我没想到的是，他竟然能够用JSON数组的格式，返回了所有的消费记录，再上传一张手绘的草图。

![](img/cd36720746771989945984d85ff95f1d_205.png)

真没想到这么抽象的一张图。

![](img/cd36720746771989945984d85ff95f1d_207.png)

他都能推理出来，而且给出了正确的原因，目前已经在GITHUB开源了。

![](img/cd36720746771989945984d85ff95f1d_209.png)

模型也托管在哈根face上面，大家可以照着示例代码部署的体验一下。

![](img/cd36720746771989945984d85ff95f1d_211.png)

好诶再往下对，然后这个是更多的一些一些例子啊。

![](img/cd36720746771989945984d85ff95f1d_213.png)

对然后接下来嗯，下一个呢，我们就就就就说我们其实还是就说希望能够，刚刚刚就是刚才最开始的有说到，其实我们的出发点啊，就就就就就说是希望他能能能够，就说是第边际成本的实现开放任务的泛化啊。

虽然说这个呃语言这样这样的一种，一种交互的方式，它已经能够实现很多的开放的，这样的一种多模态的任务，但是还有非常多的任务啊，就是说其其实他现在在这个时间点并不是很好，能够用语言来进行刻画的。

但以及其实还有非常多的这种视觉任务，可能到和未来可能都是并不是，能够很好的适用于语言这样一种模态的对嗯，然后所然后，所以呢，就是我们这开发了一个通用的一个任务，解码器啊，然后它叫叫做呃。

他更更更早的一个版版版本叫做微信LOM啊，就是我我们这这我们这个系列，我们的想法呢，就是希望能够打造一个开放的这种多多多模态，以视觉为核心的多模态任务，的一个开放的这个decoder啊。

能够低边际成本的泛化到各种各样的，千奇百怪的这种以视觉为核心的，这样一种多模态任务上，这是我们这个系列工作的这个核心嗯，然后在这个版本里面呢，啊我们这儿创我箭头全全没了呵，OK然后在我们这个版本里面呢。

就就就就是我们嗯做了一个叫做向量链接的啊，这样的一个技术啊，然后英文的话对应的名名字应该要更贴切一些，大家看看论文就就就好了，对嗯对，因为我我在PPT平平就是做做做了。

就平时朋友可能在国内的一些对一些其他场合，我我我我也会用的，所以就基本上都尽量保尽量是用中文对，然后呢呃他基基本的想法呢，就就就就就就就说是呃，我们通过跟其他的东西的对比啊，然后我们来看哇。

不过这里面啥东西都看不见了，我也不知道该怎么讲啊，呵OK这就是我我我举举举举个例子吧，最左最左边这这这这个图呢就就是一种啊，以前经常用的就是用大圆模型为核心，去调不同工具的这样的一种做法呃。

以这个vision叫以visual chat gchat g p t为代表的啊，它就是以大圆模模模型作为一个agent，作为一个调用不同工具的一个核心，然后呢他会调，比如说已经已有的。

比如说上百个视觉的，比如说detection semitation，pose estimation啊，都还有，还有image generation等等不同的工具，它跟这些工具之间。

它这个链接其实最核心的是中间画的这个画的，那个那那那个线，但但是这个这个PPT的版本，版本就看不出来了，就是他跟他们之间的调用的接口，就只是一个是以语言和文字为接接接口，就是就是他会给你发一个指令。

说我调用你，然后给你这个模模型传入些什么参数，这个模这个模模型视觉的模模型做完事之后，他把这个检测到的框，或者说是这个估计到的pose或怎么着，就直接反，就把它接。

最终的结果返回给这个语言的这个指令中枢啊，它它们之间是这样一种比较松耦合的一种方式，并不能end to end的进行训练，也并不能够传feature啊，所以呢呃它的整个性能的上限是比较低的。

然后还有一种方式呢，就是中中间的这样的一种方式，它是就是它它它它它是一种紧紧耦合啊，就是视觉，就是说这个呃多模态的这个大模型，跟你某种扩展的能力。

比如说object detection或者是SEMITATION，或者是这个图像内容声生成，或者是视频声声生成啊，它是一个呃多多模态的大模型，跟一个专有的工具之间，它们形成一个特征的紧耦合的一个关系啊。

但但是呢它他们这类方法的问题呢，是他就是呃并不通用啊，就是我一个大一个大多模态模型，就绑定一个特定的工具，然后然后end tw的去训练啊，你你你就是说最后弄完之后啊。

这样的一套模型和工具大家就绑定在一起了，它并不具有开放式任务的泛化的这样一个能力，然后我们在这样这样的一一个方法呢，就是向量链接，它中间是有一个路由的一个机制啊，就是我们就是一个多模态大模型为中枢啊。

它可以向外去扩展，调用上百个或者几几百个也也行啊，各种各样的视觉或者多模态的工具，它们之间通过路由啊，它它这个多模态模模型，它它自己确定根据刚才的指令，我应该调用什么样的工具，然后呢。

并且呢它们之间的连接呢，是这个feature层面的这个连接啊，然后feature层面的连的连接呢，那你有可可就是这个大圆模型和工具之间，他们的这个传输的带宽会非常宽，然后以及在训练的时候。

你可以ENTTW的进行训练，所以呢就是具备呃，就上面两两种以前的方法的好处，啊这这这边这个图也看不了了，对，然后呢嗯说这这这这这最左边呢，其实我们是对各各种，就是现现现在大员模模就是单纯只用通过语言。

通过interface并不能够很好处理的，可能几十个上百个任务，我们把它跟根据这种不同的维度，把它给放在这个做坐坐标系上上上上上面啊，他们在不同的维度上，他们这些任务的难度是怎么样的啊。

然后呢所以我们这样以一个模型呢，就就就就就能够去覆盖掉呃，非常多的多种多样的这种多模态的任务啊，然后能够应用在不同的场景里面对啊，这呢是是一些例子，然后这是这个呢是这个算法的，这个项目的二维码啊。

已经开已经那个放出来了对，然后再然后这这个呢是是就是说呃，对在在在场的各位都很都很熟悉啊，大家都是以前都是wobject detection的对。

然后这个呢是调这个object detection的工具啊，然后来去去这个去去处理一些各种各样的，复杂的一些一些场景，对通过调这种工具呢，它能够对做很好的各种场景下的这种检测。

然后在这在这个是你问他一些，别别别的一些问题啊，就是说你让他说呃，比如比如说你问问他，你说你希望就是定位图片里面所有的人啊，然后呢你让让让他把那个QQ17个key point，这种标注。

把这里面所有的把都都给标出来啊，然后或或者你只只只只做一一个点位啊，要定位所有的人，然后呢你你把他这个right elbow啊，右右走的位置把他给点出来，或者是干些其其他的事情啊，都可以对。

好然后这这这这个商要让他去去去去那个调，这个就是呃image editing，或或或或或或者是图像生成啊，这样这样的一些工具来进行，进行图像的一些编辑，这样的一些操作啊。

最后呢啊是我们往这个呃就是与世界进行交，进行交互啊这个方向去走的一些尝试，那这个其实其实是相相对更早一些的，这个工作是我们去年5月份的时候哦，PUBLIPUBLIC出来的对。

然后我们我们其实在在在在呃去年的时候，去年年初的时候啊，那会就非常敏锐的就说意识到了，就就就说这个呃跟我就是这种嗯大模型，它的威力会拓展到跟这个虚拟环境，还有现实是现实环境中间的交互里面去。

就以前基于强化，纯粹基于强化学习的方法，有太多的这个缺点和问题啊，然后大圆模模型或者是大模型，它能够很好的弥补这样的缺点，所以我们在去年年初的时候啊，就开启了这样一个项目啊。

然后去尝试去去玩一个最开放的，然后卖的最好的这个游戏MINECRAFT啊，我的世界，然后呢在这个里面呢啊，然后我们就在MINECRAFT里面啊，最最最最早啊，就说跟英伟达的这个voyager2者同期啊。

这两工作最最早在这个我的世界里面证明了，这种大圆基于大圆模，大圆模型的智能体，相较于以前的强化学习的智能体啊，它具有一个非常强的一个泛化，还有这样的一个智能化的这样的一个能力。

对啊这个是我今天整个talk啊。

![](img/cd36720746771989945984d85ff95f1d_215.png)