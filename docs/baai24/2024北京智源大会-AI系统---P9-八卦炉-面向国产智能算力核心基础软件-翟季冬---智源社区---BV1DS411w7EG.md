# 2024北京智源大会-AI系统 - P9：八卦炉-面向国产智能算力核心基础软件-翟季冬 - 智源社区 - BV1DS411w7EG

感谢林院长的介绍，我快点讲，要不然大家该饿肚子了，咱们已经12点了，后面还有一个报告，我今天的报告题目是。



![](img/7794d5f137fa1134d360402c17ff24ba_1.png)

面向国产智能算力的核心基础软件，其实主要是想介绍一下我们实验室，在最近几年做的一些工作，我觉得大的背景就不介绍，因为整个今天我们的工作都是，今天上午我也认真学习了，每位老师的报告。

都是围绕人工智能去展开，我觉得其实这里稍微强调一点，其实最近几年基于转换器的大模型，其实它对算力产生爆发式的需求，其实不只是在模型的研发，然后训练，然后包括微调到推理，其实整个过程我觉得。

其实都产生了非常强的需求，所以说其实这一波人工智能，最赚钱的企业就是英伟达，反正不知道人工智能最后能不能落地，至少英伟达先把钱给赚去，因为大家不管是训练的微调，都需要算力，我觉得这块我给了一些数据。

这个算力实际上是，目前这波大模型产业的主要开销，在训练的过程中，大家还要投入很多钱在数据的清洗，但是当模型训练好之后，当你部署起来的时候，其实推理的成本主要是算力的开销，所以说包括刚才向靳辉介绍。

其实像他企业其实轨迹流动，朝着推理，其实我觉得也是一个非常重要的方向，然后我们就来看一下，其实包括今天上午，智原包括裕隆的报告，其实也在国内算力，大家知道其实现在，我们其实还是在公开的渠道。

还是很难拿到国外的算力，包括像英伟达的高端算力，包括像H100 A100这样的高端算力，然后其实我们中国，其实现在目前也在做国产算力，其实有很多公司，包括像华为 天数 沐昕等等，然后但是其实国产算力。

其实我觉得大家，我们也就是包括今天我们在座的同仁，其实也在国产算力去共同努力，但实际上就是说，国产算力当我们去用的时候，还是存在着一些，就是说潜在的我们需要改进的地方，我觉得这也是我们其实未来几年。

需要去共同努力的地方，我觉得其中有一块，就是这个底层的这个基础软件，我们如何能够把更多的，这个人工智能的一些模型，在国产算力上能把它发挥的极致，我觉得这也是我们其实，要共同努力去做的。

然后其实在这个大的背景下，大家知道其实中国，其实现在在很多省市，其实已经在最近几年，去建一些国产的，质算超算的算力中心，我觉得这也是未来几年，其实一个非常大的一个市场，其实这些大的这些质算和超算中心。

其实大家如果去看的话，基本上是以国产的算力为主，然后其实我们能看到，其实一个非常有意思现象，就是说大家跟大模型的，这些相关的企业聊，其实大家都是需要这个算力，但实际上建的这个，很多的一些质算中心呢。

其实它算力资源呢，其实利用的并不是很充分，我觉得中间呢，还是有很大的一个gap，那为什么呢，其实我觉得这里的一个核心挑战，就是这个底层的这个算力软件生态，我觉得这个其实大家可以看一下，这个其实现在呢。

其实在英伟达这个体系，其实包括今天我们很多报告都聊英伟达，其实英伟达呢，其实我觉得最早它发展其实这个软件呢，其实当它开始从游戏市场进入这个算力，实际上大约是04 05年，当时我还在读研究生，然后当时呢。

其实英伟达在给包括清华很多学校送卡，他觉得说哎我们是不是说，其实可以用英伟达的卡做一点计算，然后当时呢，我们实验室有一个跟我差不多一起读书的，然后当时说哎考虑是不是做矩阵成，然后英伟达就给打印了几本书。

说哎怎么在这个英伟达平台上去写矩阵成，当时说哎要写，要看这么厚的一本书，其实这个编程很复杂，然后后来呢，其实大约在07 08年的时候，CUDA慢慢出来，然后到后面其实这个这一波人工智能起来之后。

是非常快速地推动了英伟达发展，然后我们看到其实在英伟达上面，其实从最上面，其实大家用hack and face，可以很方便地去用很多模型，这个包括训练微调，其实它实际上是把PyTorch。

其实好多做模型的人会觉得说，我用PyTorch写代码还是会很复杂，但是我们做系统软件是往下，其实有这个Kubelus，包括今天其实大家基于的Triton，其实整个英伟达呢，实际上把整个这套生态呢。

其实至少就是让我们在各个层次去做事情，都提供了非常好的，但实际上呢，就是说国产的，其实你跟很多芯片厂人聊，他也会说我支持hack and face，支持PyTorch，支持Tensorflow。

我有很完善的算子库，有很完善的整个底层，但实际上当你去做更多的这个优化的时候，你会发现，其实它底层呢还是有很多问题，比如说可能跑标准的拉马是没问题，但是当你把拉马改一改，或者把很多这个模型改一改的时候。

你会发现其实这里边就会问题很多，然后包括其实模型，包括很多软件的小的版本号，你会发现很不兼容，我觉得这是一个非常大的一个挑战，其实我这里经常举的一个例子是，其实所谓的生态好不好，比如说像在清华里边。

还有很多非计算机专业，比如说物理系化学系，大家知道其实现在做AFO Science，对人工智能需求非常强，那这些老师其实当他有一点经费的时候，比如说我有个几十万，我要买几块卡，那他们愿意买什么卡。

你会发现其实他们还是愿意买英伟达的卡，因为发现买了英伟达卡，其实改一改软件还是能跑，但是其实你让他们去买一个国产芯片，还是有很大的这个压力，然后大家知道，其实这里面的核心的就是说。

其实我现在人工智能模型，虽然都是可能现在目前主流模型都是Transformer，但是其实每家都会在Transformer上做一些改进，然后同时其实在架构本身也会有很多变化，那这个时候其实大家知道。

即使在英伟达上面，那你要把这些模型其实跑得很好，我觉得这个也需要很多的努力，但是可能在国产的卡上把这些模型跑好，其实就更需要更多的核心的系统软件的研发人员，其实刚才其实连长也提到。

其实我觉得像系统软件在中国，其实还是蛮缺这方面人才，包括像我在清华带的这些博士生，基本上应届生就可以拿到200万的这个package，就是200万到300万，就说明其实这个方向其实还是非常缺人。

就是说你还有很多的这个溢价的这个空间，然后我下面就总结一下，其实智能算力其实，那围绕其实我们总结，还是有大约10个非常关键的软件，就是说我们要把它做的比较好，其实大家可以从下网上去看这个。

当我们在一个比较大的一个计算中心上，做模型的训练或者推理的时候，首先我们要有调度器，我们如何把这个白卡铅卡充分的调度起来，然后我们要很好的去管理内存，大家知道其实我要做模型的训练推理。

我都要高效的去管理内存，然后同时呢其实做模型的训练，你要有高效的容错的系统，然后同时呢大家知道，其实我做模型训练微调，我要读大量的数据，那你底层要有比较好的，并行文件系统的这个知识。

然后再往上到了这个芯片层，你要有很好的编程语言，其实刚才我们提到，比如像英伟达有CUDA，那刚才其实这个谢老师提到，比如说像Cycle，其实Cycle呢我觉得，就是说也是一个目前在发展非常迅速的。

就是说其实我们如果做更多的芯片，我们肯定不希望大家，要么完全自己做一套新的编程语言，跟别人不兼容，然后在编程语言之上呢，其实很重要的一个环节就是编译器，今天有很多老师有提到，就是说其实如何把这些算子。

高效的编译到底层，其实这里是一个非常重要的一环，然后还有算子库，其实算子库跟编译呢，其实是在一个level，就是说我们可以把常见的这些算子，是这个编译成高效算子，但是还有很多常委的分布。

那你一定需要去编辑器支撑，然后再往上呢，其实当你涉及到多卡多机的时候，那这个时候通信，其实今天是刚刚也提到了，就是说其实当你在多机多卡的时候，那这个时候通信就非常的关键，然后编程框架呢。

实际上是把这些整合起来，就是说当然就是说，其实它给我们提供了一个，很好的用户接口，当然在底层之下呢，我们都可以去做很多，我们自己的替代，然后当你在大规模的时候，其实不管是训练还是推理。

其实那你对并行的这个需求，就会非常大，然后这里呢，其实我讲一下，我们其实实验室在做的，这个看的问题的角度，就是说底层呢有国产的这个芯片，我觉得这里其实有一层的核心的关键，就是编译器。

那你如何把底层的这些国产的芯片，能把这些性能发挥的极致，那编译器首先是一个非常重要的环节，不管是AI的芯片，包括像RISC-V，包括我们做各种各样的芯片，我觉得这也是，其实大家看这个计算机系。

其实四大原理课程，有操作系统原理，组成原理，网络原理，再就是编译原理，就是能教成原理的课，在计算机系只有这四门课，其实是非常重要的一个环节，然后再往上的核心其实就并行，那你如何其实能够。

比如说现在的M1模型，比如说这个都很大，那我们如何在比如说单机8卡，把这个模型推理做得很好，那这个时候并行就是非常关键，然后当你做这个白卡，千卡或者万卡的训练的时候，那并行也非常重要，那实际上这两层。

其实我们都可以在PiTorch之下去把它去改掉，也就是说让用户，其实去不改变代码，同时能够充分发挥底层算力的性能，然后这个里边紫色部分，实际上是我在清华带的这个实验室过去几年，包括在这个容错。

包括在调度，然后底层的编程语言，然后编译器方向，然后包括上层并行，我们做的一些工作，然后因为时间原因呢，我下面呢其实简单的介绍一下，我们其实在这几个方向做过的一些工作，大家如果感兴趣呢。

其实可以去网上下载，我们其实很多系统我们都是开源，我介绍第一个工作呢是在编程语言，大家知道其实这个编程语言呢，其实就是说其实现在在这个ego芯片呢，其实做领域特定的编程语言是一个非常有意义的。

因为实际上我们不能让一个编程语言去解决全部问题，然后我们这个工作叫free tensor，它的核心就是说，我们有一类的人工智能模型，它实际上是不规则的，比如说我大家看右边的数据，我给了四个模型。

最右边叫JT，实际上是这个图，就是图神经网络，GN这类的，它本身是一个西数问题，然后longformal，实际上就是这个long sequence，就是大家知道现在大模型处理这个长序列。

那我会在这个会有很多这种算法，那它本身呢其实它也不是一个规整的，然后此外的还有这个softras，softras它实际上就是拍几个二维照片，然后帮你生成三维的图片，那它这里面也是一个不规则。

那针对这个呢我们在PyTorch里面做了些扩展，然后做了这个很多的性能的这个优化，然后跟PyTorch原生比呢，其实在这个英伟达平台，我们可以有上百倍的性能提升，这个主要是讲不规则的模型。

其实这里边会有很大的这个性能提升的空间，然后这个工作呢我们也在Github开源，这个感兴趣这个大家可以去下载，然后第二个工作呢其实就编译器，其实今天有好多老师提到编译器，我觉得就是说。

我们要把底层的这个芯片呢，其实算力发挥起来，就是即使像英伟达的平台，那我们如果在编译器做很多事情，其实还是能提升，比如说30%或50%的这个性能，那在PyTorch之下呢，其实今天有老师讲。

就是说它的核心呢，其实为了发挥性能，像PyTorch会把这个Eager Mode，其实这个执行方式会转成一个计算图，这样的话可以在编译层面做很多优化，也就是说其实今天上午的一些报告提到。

比如说有图层的编译和底层算子编译，然后我们这个工作呢叫i-Knight，实际上是去年的OSDi的文章，我们实际上是把这个图层跟算层打开，这两层合到一块，也就是说可以挖掘出更多的这个优化的空间。

就是之前呢比如说像有些图层的优化，那是在这个图，比如做一些Chrono Fusion，然后在算子，其实今天包括赵杰老师讲的，包括算子其实也做很多优化，我们这个核心呢实际上是把图层跟算层。

融合到一起来挖掘更多的优化空间，我们这个是在英伟达的A版上面，然后做了一些模型，然后包括一些经典的这种卷迹类的，和现在的这种Transformer Base。

然后跟TensorFlow还有TensorRT，TensorRT是英伟达上面一个默认的编译化的工具，然后Py的实际上是我们之前21年的一个工作，然后最多呢其实也会有将近两倍的性能提升。

其实这些想跟大家讲，就是说即使是英伟达平台，我觉得你去深入的挖掘底层的编译，包括上层的，其实还是会有很多这个性能提升的空间，然后最近呢其实我们在这个大圆模型，大家知道其实刚才那个。

近回讲到就是说其实在推理，其实它是一个Memory Bound，那实际上就是说大家知道我们要把模型参数，然后包括中间的像KVCache存起来，那这个其实对内存会有非常大压力，然后它对这个系统内存带宽。

会有非常高的这个要求，然后我们这个系统的核心呢，我们把模型参数和KVCache给它分离，把KVCache呢给它挪到CPU的内存，这样的话，然后通过这样的流水线的并行。

来充分的发挥CPU跟GPU的计算能力，这样的好处就是说，我Batch Size就不再受到KVCache的内存，占用的这个限制，然后同时我还可以把CPU的内存带宽，跟GPU的内存带宽整合起来。

来共同的提高这个吞吐，我们这个是在英伟达平台，跟VLM，TensorRT LLM系统比，我们Batch Size可以提升到百倍，然后GPU吞吐量提升1。8到14倍，这个系统我们最近也会把它开源。

这个系统叫Fast Decode，然后呢其实我们在这个训练测呢，其实我们一直在维护就是MoE，其实MoE呢在21年，其实年初的时候还没有火的时候，我们就开始在PyTorch里面。

做了这个MoE的并行知识，也就是说你在PyTorch里面加一行代码，我们就可以自动的帮你去做各种并行，我们这个系统是21年开源，叫Fast MoE，后来我们又持续的更新，然后那个美国的Facebook。

它在PyTorch开源应该是比我们还要晚，一两个月的时间，我们这个系统呢也包括支撑了，北京智原的悟道的这个大模型的训练，然后我们后来呢其实把这个系统持续的改进，包括Fast MoE是这个。

是我们在PyTorch R上的一个工作，我们这个跟Deep Speed MoE系统比，可以比它快高达十几倍的这个性能，然后我们去年呢又进一步的优化，做了一个Smart MoE系统。

然后能比我们这个Fast MoE这个系统，又进一步提升将近两倍的性能，就是这些数据都是在英伟达的平台去测的，然后下面讲一下在这个国产算力方面，就是说大模型的大家知道训练呢，其实它对这个。

其实硬件呢还是提出很高的这个要求，然后这个国内呢其实青岛这台机器呢，它是一个纯国产的，大约有十万个节点，大家可以列成有十万个国产的这个加速的卡，它的性能呢其实相当于是1。8万块的。

英伟达的A100还是蛮大的一个系统，然后我们其实在这个系统上呢，其实我们是从底层的编译，然后包括像内存管理，然后包括多机的通信，我们其实是把整个的这套系统，其实这个一致起来。

然后可以就是说把PyTorch代码，可以在这个系统上去跑起来，我们把我们整个这套软件叫做八卦炉，就是包括底层的这个通信编译，然后最后是训练起一个百万亿，大家知道其实现在呢，国内大家都在训练万亿模型。

其实是万亿的100倍，然后我们最近也支持这个，包括像AFO Science的一些，大家在这个系统上做一些AFO Science的，也在支撑这样的这个系统，然后我们目前呢，也把百川包括拉玛一些模型。

在这个青岛的平台支撑起来，然后至少可以在这个，国产的这个系统上做，包括百川拉玛的这些模型的训练和推力，然后我这有一个视频，我不知道是不是能播放出来，可能这个系统没有声音没关系，就是说这里边其实有声音。



![](img/7794d5f137fa1134d360402c17ff24ba_3.png)

就是说我们其实主要想讲，就是说其实用这个国产的这个平台呢。

![](img/7794d5f137fa1134d360402c17ff24ba_5.png)

我目前已经可以把这个，包括百川还有拉玛，然后这个支撑起来。

![](img/7794d5f137fa1134d360402c17ff24ba_7.png)

然后因为其实这个国产超算呢，实际上是国家投钱建的，然后它本身的这个。

![](img/7794d5f137fa1134d360402c17ff24ba_9.png)

就相当于基石费呢还是很便宜，但如果在这个平台上做训练和推力，其实还是要凭你去租。

![](img/7794d5f137fa1134d360402c17ff24ba_11.png)

这个英伟达的A100或H100的话，其实成本会低很多。

![](img/7794d5f137fa1134d360402c17ff24ba_13.png)

然后最后总结一下，我应该还提前了三分钟，就是说我觉得其实中国呢，现在发展人物智能领域，其实我觉得构建国产的智能算力，我觉得还是非常重要的，就是说因为整个美国，对其实中国呢还是封锁，然后在这个一环里面。

其实我一直还是坚信，其实包括在做，就是说发展这个底层的基础软件，还是非常重要，就是我们要把，就是说其实，其实大家如果去看英伟达的话，其实英伟达其实它很多的工作，都是在软件层次去做。

我觉得这也就是说前面讲到，包括第一个，这个我们这个国外老师的这个talk，其实他也提到，就是说其实这个，我们要把这个底层算力发挥起来，其实这个软件呢，实际上是站到非常重要的一环，那就是说我们能够。

构建我们国内一个比较好的，这个良好的软件生态，可以降低大模型，在不同AI芯片的这个适配开销，好 谢谢大家，謝謝大家。



![](img/7794d5f137fa1134d360402c17ff24ba_15.png)

謝謝大家。